{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = load_dataset(\"wikimedia/wikipedia\", \"20231101.az\")[\"train\"].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"/Users/eljan/Documents/azGPT/data/corpus-raw/chunks.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = raw_datasets['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1[['text']]\n",
    "df = df[['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, df1], ignore_index=True, axis=0)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the dataset\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "# train, validation split\n",
    "train_df = df[:int(0.9*len(df))]\n",
    "val_df = df[int(0.9*len(df)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to huggingface dataset and upload to the hub\n",
    "from datasets import Dataset, DatasetDict\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "dataset = DatasetDict({\"train\": train_dataset, \"val\": val_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset.push_to_hub(\"eljanmahammadli/wiki_and_chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "old_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-13b-hf\")\n",
    "\n",
    "def get_training_corpus():\n",
    "    dataset = raw_datasets[\"train\"]\n",
    "    for start_idx in range(0, len(dataset), 1000):\n",
    "        samples = dataset[start_idx : start_idx + 1000]\n",
    "        yield samples[\"text\"]\n",
    "\n",
    "training_corpus = get_training_corpus()\n",
    "tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 32000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(\"azGPT-LLaMA-tokenizer-perplexity\")\n",
    "tokenizer.push_to_hub(\"azGPT-perplexity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = raw_datasets['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "df['token_length'] = df['text'].progress_apply(lambda x: len(tokenizer(x)['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['token_length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['token_length'] < 1024]['token_length'].plot(kind='hist', bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['token_length'] < 512].shape[0] / df.shape[0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import math\n",
    "\n",
    "original_vocab_size = len(tokenizer)\n",
    "\n",
    "# Calculate the padded vocabulary size (e.g., to the next multiple of 8)\n",
    "def pad_vocab_size_to_multiple(vocab_size, multiple_of):\n",
    "    return math.ceil(vocab_size / multiple_of) * multiple_of\n",
    "\n",
    "padded_vocab_size = pad_vocab_size_to_multiple(original_vocab_size, 8)\n",
    "\n",
    "print(f\"Original Vocabulary Size: {original_vocab_size}\")\n",
    "print(f\"Padded Vocabulary Size: {padded_vocab_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"\")\n",
    "\n",
    "# train val split the dataset randomly\n",
    "\n",
    "df = ds['train'].to_pandas()\n",
    "\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "train_df = df[:int(0.9*len(df))]\n",
    "val_df = df[int(0.9*len(df)):]\n",
    "\n",
    "# convert back to huggingface dataset\n",
    "from datasets import Dataset, DatasetDict\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "dataset = DatasetDict({\"train\": train_dataset, \"val\": val_dataset})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "polygraf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
