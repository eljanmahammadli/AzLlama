{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"eljanmahammadli/AzLlama-152M-Alpaca\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"eljanmahammadli/AzLlama-152M-Alpaca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaModel(\n",
       "  (embed_tokens): Embedding(32000, 768)\n",
       "  (layers): ModuleList(\n",
       "    (0-11): 12 x LlamaDecoderLayer(\n",
       "      (self_attn): LlamaSdpaAttention(\n",
       "        (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (k_proj): Linear(in_features=768, out_features=256, bias=False)\n",
       "        (v_proj): Linear(in_features=768, out_features=256, bias=False)\n",
       "        (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=768, out_features=3072, bias=False)\n",
       "        (up_proj): Linear(in_features=768, out_features=3072, bias=False)\n",
       "        (down_proj): Linear(in_features=3072, out_features=768, bias=False)\n",
       "        (act_fn): SiLU()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm()\n",
       "      (post_attention_layernorm): LlamaRMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (norm): LlamaRMSNorm()\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed_tokens.weight: 24576000 parameters\n",
      "layers.0.self_attn.q_proj.weight: 589824 parameters\n",
      "layers.0.self_attn.k_proj.weight: 196608 parameters\n",
      "layers.0.self_attn.v_proj.weight: 196608 parameters\n",
      "layers.0.self_attn.o_proj.weight: 589824 parameters\n",
      "layers.0.mlp.gate_proj.weight: 2359296 parameters\n",
      "layers.0.mlp.up_proj.weight: 2359296 parameters\n",
      "layers.0.mlp.down_proj.weight: 2359296 parameters\n",
      "layers.0.input_layernorm.weight: 768 parameters\n",
      "layers.0.post_attention_layernorm.weight: 768 parameters\n",
      "layers.1.self_attn.q_proj.weight: 589824 parameters\n",
      "layers.1.self_attn.k_proj.weight: 196608 parameters\n",
      "layers.1.self_attn.v_proj.weight: 196608 parameters\n",
      "layers.1.self_attn.o_proj.weight: 589824 parameters\n",
      "layers.1.mlp.gate_proj.weight: 2359296 parameters\n",
      "layers.1.mlp.up_proj.weight: 2359296 parameters\n",
      "layers.1.mlp.down_proj.weight: 2359296 parameters\n",
      "layers.1.input_layernorm.weight: 768 parameters\n",
      "layers.1.post_attention_layernorm.weight: 768 parameters\n",
      "layers.2.self_attn.q_proj.weight: 589824 parameters\n",
      "layers.2.self_attn.k_proj.weight: 196608 parameters\n",
      "layers.2.self_attn.v_proj.weight: 196608 parameters\n",
      "layers.2.self_attn.o_proj.weight: 589824 parameters\n",
      "layers.2.mlp.gate_proj.weight: 2359296 parameters\n",
      "layers.2.mlp.up_proj.weight: 2359296 parameters\n",
      "layers.2.mlp.down_proj.weight: 2359296 parameters\n",
      "layers.2.input_layernorm.weight: 768 parameters\n",
      "layers.2.post_attention_layernorm.weight: 768 parameters\n",
      "layers.3.self_attn.q_proj.weight: 589824 parameters\n",
      "layers.3.self_attn.k_proj.weight: 196608 parameters\n",
      "layers.3.self_attn.v_proj.weight: 196608 parameters\n",
      "layers.3.self_attn.o_proj.weight: 589824 parameters\n",
      "layers.3.mlp.gate_proj.weight: 2359296 parameters\n",
      "layers.3.mlp.up_proj.weight: 2359296 parameters\n",
      "layers.3.mlp.down_proj.weight: 2359296 parameters\n",
      "layers.3.input_layernorm.weight: 768 parameters\n",
      "layers.3.post_attention_layernorm.weight: 768 parameters\n",
      "layers.4.self_attn.q_proj.weight: 589824 parameters\n",
      "layers.4.self_attn.k_proj.weight: 196608 parameters\n",
      "layers.4.self_attn.v_proj.weight: 196608 parameters\n",
      "layers.4.self_attn.o_proj.weight: 589824 parameters\n",
      "layers.4.mlp.gate_proj.weight: 2359296 parameters\n",
      "layers.4.mlp.up_proj.weight: 2359296 parameters\n",
      "layers.4.mlp.down_proj.weight: 2359296 parameters\n",
      "layers.4.input_layernorm.weight: 768 parameters\n",
      "layers.4.post_attention_layernorm.weight: 768 parameters\n",
      "layers.5.self_attn.q_proj.weight: 589824 parameters\n",
      "layers.5.self_attn.k_proj.weight: 196608 parameters\n",
      "layers.5.self_attn.v_proj.weight: 196608 parameters\n",
      "layers.5.self_attn.o_proj.weight: 589824 parameters\n",
      "layers.5.mlp.gate_proj.weight: 2359296 parameters\n",
      "layers.5.mlp.up_proj.weight: 2359296 parameters\n",
      "layers.5.mlp.down_proj.weight: 2359296 parameters\n",
      "layers.5.input_layernorm.weight: 768 parameters\n",
      "layers.5.post_attention_layernorm.weight: 768 parameters\n",
      "layers.6.self_attn.q_proj.weight: 589824 parameters\n",
      "layers.6.self_attn.k_proj.weight: 196608 parameters\n",
      "layers.6.self_attn.v_proj.weight: 196608 parameters\n",
      "layers.6.self_attn.o_proj.weight: 589824 parameters\n",
      "layers.6.mlp.gate_proj.weight: 2359296 parameters\n",
      "layers.6.mlp.up_proj.weight: 2359296 parameters\n",
      "layers.6.mlp.down_proj.weight: 2359296 parameters\n",
      "layers.6.input_layernorm.weight: 768 parameters\n",
      "layers.6.post_attention_layernorm.weight: 768 parameters\n",
      "layers.7.self_attn.q_proj.weight: 589824 parameters\n",
      "layers.7.self_attn.k_proj.weight: 196608 parameters\n",
      "layers.7.self_attn.v_proj.weight: 196608 parameters\n",
      "layers.7.self_attn.o_proj.weight: 589824 parameters\n",
      "layers.7.mlp.gate_proj.weight: 2359296 parameters\n",
      "layers.7.mlp.up_proj.weight: 2359296 parameters\n",
      "layers.7.mlp.down_proj.weight: 2359296 parameters\n",
      "layers.7.input_layernorm.weight: 768 parameters\n",
      "layers.7.post_attention_layernorm.weight: 768 parameters\n",
      "layers.8.self_attn.q_proj.weight: 589824 parameters\n",
      "layers.8.self_attn.k_proj.weight: 196608 parameters\n",
      "layers.8.self_attn.v_proj.weight: 196608 parameters\n",
      "layers.8.self_attn.o_proj.weight: 589824 parameters\n",
      "layers.8.mlp.gate_proj.weight: 2359296 parameters\n",
      "layers.8.mlp.up_proj.weight: 2359296 parameters\n",
      "layers.8.mlp.down_proj.weight: 2359296 parameters\n",
      "layers.8.input_layernorm.weight: 768 parameters\n",
      "layers.8.post_attention_layernorm.weight: 768 parameters\n",
      "layers.9.self_attn.q_proj.weight: 589824 parameters\n",
      "layers.9.self_attn.k_proj.weight: 196608 parameters\n",
      "layers.9.self_attn.v_proj.weight: 196608 parameters\n",
      "layers.9.self_attn.o_proj.weight: 589824 parameters\n",
      "layers.9.mlp.gate_proj.weight: 2359296 parameters\n",
      "layers.9.mlp.up_proj.weight: 2359296 parameters\n",
      "layers.9.mlp.down_proj.weight: 2359296 parameters\n",
      "layers.9.input_layernorm.weight: 768 parameters\n",
      "layers.9.post_attention_layernorm.weight: 768 parameters\n",
      "layers.10.self_attn.q_proj.weight: 589824 parameters\n",
      "layers.10.self_attn.k_proj.weight: 196608 parameters\n",
      "layers.10.self_attn.v_proj.weight: 196608 parameters\n",
      "layers.10.self_attn.o_proj.weight: 589824 parameters\n",
      "layers.10.mlp.gate_proj.weight: 2359296 parameters\n",
      "layers.10.mlp.up_proj.weight: 2359296 parameters\n",
      "layers.10.mlp.down_proj.weight: 2359296 parameters\n",
      "layers.10.input_layernorm.weight: 768 parameters\n",
      "layers.10.post_attention_layernorm.weight: 768 parameters\n",
      "layers.11.self_attn.q_proj.weight: 589824 parameters\n",
      "layers.11.self_attn.k_proj.weight: 196608 parameters\n",
      "layers.11.self_attn.v_proj.weight: 196608 parameters\n",
      "layers.11.self_attn.o_proj.weight: 589824 parameters\n",
      "layers.11.mlp.gate_proj.weight: 2359296 parameters\n",
      "layers.11.mlp.up_proj.weight: 2359296 parameters\n",
      "layers.11.mlp.down_proj.weight: 2359296 parameters\n",
      "layers.11.input_layernorm.weight: 768 parameters\n",
      "layers.11.post_attention_layernorm.weight: 768 parameters\n",
      "norm.weight: 768 parameters\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# Function to count parameters per layer\n",
    "def get_parameters_per_layer(model):\n",
    "    param_info = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            param_info.append((name, param.numel()))\n",
    "    return param_info\n",
    "\n",
    "# Getting parameter details\n",
    "parameter_details = get_parameters_per_layer(model.model)\n",
    "\n",
    "# Printing parameter details (Python)\n",
    "for name, num_params in parameter_details:\n",
    "    print(f\"{name}: {num_params} parameters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128404224"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get total number of parameters\n",
    "total_params = sum([num_params for name, num_params in parameter_details])\n",
    "total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}[h]\n",
      "\\centering\n",
      "\\begin{tabular}{|c|c|}\n",
      "\\hline\n",
      "\\textbf{Layer Name} & \\textbf{Number of Parameters} \\\\\n",
      "\\hline\n",
      "embed\\_tokens.weight & 24576000 \\\\\n",
      "\\hline\n",
      "layers.0.self\\_attn.q\\_proj.weight & 589824 \\\\\n",
      "\\hline\n",
      "layers.0.self\\_attn.k\\_proj.weight & 196608 \\\\\n",
      "\\hline\n",
      "layers.0.self\\_attn.v\\_proj.weight & 196608 \\\\\n",
      "\\hline\n",
      "layers.0.self\\_attn.o\\_proj.weight & 589824 \\\\\n",
      "\\hline\n",
      "layers.0.mlp.gate\\_proj.weight & 2359296 \\\\\n",
      "\\hline\n",
      "layers.0.mlp.up\\_proj.weight & 2359296 \\\\\n",
      "\\hline\n",
      "layers.0.mlp.down\\_proj.weight & 2359296 \\\\\n",
      "\\hline\n",
      "layers.0.input\\_layernorm.weight & 768 \\\\\n",
      "\\hline\n",
      "layers.0.post\\_attention\\_layernorm.weight & 768 \\\\\n",
      "\\hline\n",
      "layers.1.self\\_attn.q\\_proj.weight & 589824 \\\\\n",
      "\\hline\n",
      "layers.1.self\\_attn.k\\_proj.weight & 196608 \\\\\n",
      "\\hline\n",
      "layers.1.self\\_attn.v\\_proj.weight & 196608 \\\\\n",
      "\\hline\n",
      "layers.1.self\\_attn.o\\_proj.weight & 589824 \\\\\n",
      "\\hline\n",
      "layers.1.mlp.gate\\_proj.weight & 2359296 \\\\\n",
      "\\hline\n",
      "layers.1.mlp.up\\_proj.weight & 2359296 \\\\\n",
      "\\hline\n",
      "layers.1.mlp.down\\_proj.weight & 2359296 \\\\\n",
      "\\hline\n",
      "layers.1.input\\_layernorm.weight & 768 \\\\\n",
      "\\hline\n",
      "layers.1.post\\_attention\\_layernorm.weight & 768 \\\\\n",
      "\\hline\n",
      "layers.2.self\\_attn.q\\_proj.weight & 589824 \\\\\n",
      "\\hline\n",
      "layers.2.self\\_attn.k\\_proj.weight & 196608 \\\\\n",
      "\\hline\n",
      "layers.2.self\\_attn.v\\_proj.weight & 196608 \\\\\n",
      "\\hline\n",
      "layers.2.self\\_attn.o\\_proj.weight & 589824 \\\\\n",
      "\\hline\n",
      "layers.2.mlp.gate\\_proj.weight & 2359296 \\\\\n",
      "\\hline\n",
      "layers.2.mlp.up\\_proj.weight & 2359296 \\\\\n",
      "\\hline\n",
      "layers.2.mlp.down\\_proj.weight & 2359296 \\\\\n",
      "\\hline\n",
      "layers.2.input\\_layernorm.weight & 768 \\\\\n",
      "\\hline\n",
      "layers.2.post\\_attention\\_layernorm.weight & 768 \\\\\n",
      "\\hline\n",
      "layers.3.self\\_attn.q\\_proj.weight & 589824 \\\\\n",
      "\\hline\n",
      "layers.3.self\\_attn.k\\_proj.weight & 196608 \\\\\n",
      "\\hline\n",
      "layers.3.self\\_attn.v\\_proj.weight & 196608 \\\\\n",
      "\\hline\n",
      "layers.3.self\\_attn.o\\_proj.weight & 589824 \\\\\n",
      "\\hline\n",
      "layers.3.mlp.gate\\_proj.weight & 2359296 \\\\\n",
      "\\hline\n",
      "layers.3.mlp.up\\_proj.weight & 2359296 \\\\\n",
      "\\hline\n",
      "layers.3.mlp.down\\_proj.weight & 2359296 \\\\\n",
      "\\hline\n",
      "layers.3.input\\_layernorm.weight & 768 \\\\\n",
      "\\hline\n",
      "layers.3.post\\_attention\\_layernorm.weight & 768 \\\\\n",
      "\\hline\n",
      "layers.4.self\\_attn.q\\_proj.weight & 589824 \\\\\n",
      "\\hline\n",
      "layers.4.self\\_attn.k\\_proj.weight & 196608 \\\\\n",
      "\\hline\n",
      "layers.4.self\\_attn.v\\_proj.weight & 196608 \\\\\n",
      "\\hline\n",
      "layers.4.self\\_attn.o\\_proj.weight & 589824 \\\\\n",
      "\\hline\n",
      "layers.4.mlp.gate\\_proj.weight & 2359296 \\\\\n",
      "\\hline\n",
      "layers.4.mlp.up\\_proj.weight & 2359296 \\\\\n",
      "\\hline\n",
      "layers.4.mlp.down\\_proj.weight & 2359296 \\\\\n",
      "\\hline\n",
      "layers.4.input\\_layernorm.weight & 768 \\\\\n",
      "\\hline\n",
      "layers.4.post\\_attention\\_layernorm.weight & 768 \\\\\n",
      "\\hline\n",
      "layers.5.self\\_attn.q\\_proj.weight & 589824 \\\\\n",
      "\\hline\n",
      "layers.5.self\\_attn.k\\_proj.weight & 196608 \\\\\n",
      "\\hline\n",
      "layers.5.self\\_attn.v\\_proj.weight & 196608 \\\\\n",
      "\\hline\n",
      "layers.5.self\\_attn.o\\_proj.weight & 589824 \\\\\n",
      "\\hline\n",
      "layers.5.mlp.gate\\_proj.weight & 2359296 \\\\\n",
      "\\hline\n",
      "layers.5.mlp.up\\_proj.weight & 2359296 \\\\\n",
      "\\hline\n",
      "layers.5.mlp.down\\_proj.weight & 2359296 \\\\\n",
      "\\hline\n",
      "layers.5.input\\_layernorm.weight & 768 \\\\\n",
      "\\hline\n",
      "layers.5.post\\_attention\\_layernorm.weight & 768 \\\\\n",
      "\\hline\n",
      "layers.6.self\\_attn.q\\_proj.weight & 589824 \\\\\n",
      "\\hline\n",
      "layers.6.self\\_attn.k\\_proj.weight & 196608 \\\\\n",
      "\\hline\n",
      "layers.6.self\\_attn.v\\_proj.weight & 196608 \\\\\n",
      "\\hline\n",
      "layers.6.self\\_attn.o\\_proj.weight & 589824 \\\\\n",
      "\\hline\n",
      "layers.6.mlp.gate\\_proj.weight & 2359296 \\\\\n",
      "\\hline\n",
      "layers.6.mlp.up\\_proj.weight & 2359296 \\\\\n",
      "\\hline\n",
      "layers.6.mlp.down\\_proj.weight & 2359296 \\\\\n",
      "\\hline\n",
      "layers.6.input\\_layernorm.weight & 768 \\\\\n",
      "\\hline\n",
      "layers.6.post\\_attention\\_layernorm.weight & 768 \\\\\n",
      "\\hline\n",
      "layers.7.self\\_attn.q\\_proj.weight & 589824 \\\\\n",
      "\\hline\n",
      "layers.7.self\\_attn.k\\_proj.weight & 196608 \\\\\n",
      "\\hline\n",
      "layers.7.self\\_attn.v\\_proj.weight & 196608 \\\\\n",
      "\\hline\n",
      "layers.7.self\\_attn.o\\_proj.weight & 589824 \\\\\n",
      "\\hline\n",
      "layers.7.mlp.gate\\_proj.weight & 2359296 \\\\\n",
      "\\hline\n",
      "layers.7.mlp.up\\_proj.weight & 2359296 \\\\\n",
      "\\hline\n",
      "layers.7.mlp.down\\_proj.weight & 2359296 \\\\\n",
      "\\hline\n",
      "layers.7.input\\_layernorm.weight & 768 \\\\\n",
      "\\hline\n",
      "layers.7.post\\_attention\\_layernorm.weight & 768 \\\\\n",
      "\\hline\n",
      "layers.8.self\\_attn.q\\_proj.weight & 589824 \\\\\n",
      "\\hline\n",
      "layers.8.self\\_attn.k\\_proj.weight & 196608 \\\\\n",
      "\\hline\n",
      "layers.8.self\\_attn.v\\_proj.weight & 196608 \\\\\n",
      "\\hline\n",
      "layers.8.self\\_attn.o\\_proj.weight & 589824 \\\\\n",
      "\\hline\n",
      "layers.8.mlp.gate\\_proj.weight & 2359296 \\\\\n",
      "\\hline\n",
      "layers.8.mlp.up\\_proj.weight & 2359296 \\\\\n",
      "\\hline\n",
      "layers.8.mlp.down\\_proj.weight & 2359296 \\\\\n",
      "\\hline\n",
      "layers.8.input\\_layernorm.weight & 768 \\\\\n",
      "\\hline\n",
      "layers.8.post\\_attention\\_layernorm.weight & 768 \\\\\n",
      "\\hline\n",
      "layers.9.self\\_attn.q\\_proj.weight & 589824 \\\\\n",
      "\\hline\n",
      "layers.9.self\\_attn.k\\_proj.weight & 196608 \\\\\n",
      "\\hline\n",
      "layers.9.self\\_attn.v\\_proj.weight & 196608 \\\\\n",
      "\\hline\n",
      "layers.9.self\\_attn.o\\_proj.weight & 589824 \\\\\n",
      "\\hline\n",
      "layers.9.mlp.gate\\_proj.weight & 2359296 \\\\\n",
      "\\hline\n",
      "layers.9.mlp.up\\_proj.weight & 2359296 \\\\\n",
      "\\hline\n",
      "layers.9.mlp.down\\_proj.weight & 2359296 \\\\\n",
      "\\hline\n",
      "layers.9.input\\_layernorm.weight & 768 \\\\\n",
      "\\hline\n",
      "layers.9.post\\_attention\\_layernorm.weight & 768 \\\\\n",
      "\\hline\n",
      "layers.10.self\\_attn.q\\_proj.weight & 589824 \\\\\n",
      "\\hline\n",
      "layers.10.self\\_attn.k\\_proj.weight & 196608 \\\\\n",
      "\\hline\n",
      "layers.10.self\\_attn.v\\_proj.weight & 196608 \\\\\n",
      "\\hline\n",
      "layers.10.self\\_attn.o\\_proj.weight & 589824 \\\\\n",
      "\\hline\n",
      "layers.10.mlp.gate\\_proj.weight & 2359296 \\\\\n",
      "\\hline\n",
      "layers.10.mlp.up\\_proj.weight & 2359296 \\\\\n",
      "\\hline\n",
      "layers.10.mlp.down\\_proj.weight & 2359296 \\\\\n",
      "\\hline\n",
      "layers.10.input\\_layernorm.weight & 768 \\\\\n",
      "\\hline\n",
      "layers.10.post\\_attention\\_layernorm.weight & 768 \\\\\n",
      "\\hline\n",
      "layers.11.self\\_attn.q\\_proj.weight & 589824 \\\\\n",
      "\\hline\n",
      "layers.11.self\\_attn.k\\_proj.weight & 196608 \\\\\n",
      "\\hline\n",
      "layers.11.self\\_attn.v\\_proj.weight & 196608 \\\\\n",
      "\\hline\n",
      "layers.11.self\\_attn.o\\_proj.weight & 589824 \\\\\n",
      "\\hline\n",
      "layers.11.mlp.gate\\_proj.weight & 2359296 \\\\\n",
      "\\hline\n",
      "layers.11.mlp.up\\_proj.weight & 2359296 \\\\\n",
      "\\hline\n",
      "layers.11.mlp.down\\_proj.weight & 2359296 \\\\\n",
      "\\hline\n",
      "layers.11.input\\_layernorm.weight & 768 \\\\\n",
      "\\hline\n",
      "layers.11.post\\_attention\\_layernorm.weight & 768 \\\\\n",
      "\\hline\n",
      "norm.weight & 768 \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "\\caption{Number of parameters per layer}\n",
      "\\label{tab:parameters}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "# Function to safely format layer names for LaTeX\n",
    "def latex_safe_string(input_string):\n",
    "    return input_string.replace('_', '\\\\_')\n",
    "\n",
    "# Function to generate LaTeX table for model parameters\n",
    "def generate_latex_table(model):\n",
    "    latex_code = \"\\\\begin{table}[h]\\n\\\\centering\\n\\\\begin{tabular}{|c|c|}\\n\\\\hline\\n\"\n",
    "    latex_code += \"\\\\textbf{Layer Name} & \\\\textbf{Number of Parameters} \\\\\\\\\\n\\\\hline\\n\"\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            # Ensure layer names are LaTeX safe\n",
    "            safe_name = latex_safe_string(name)\n",
    "            latex_code += f\"{safe_name} & {param.numel()} \\\\\\\\\\n\\\\hline\\n\"\n",
    "    \n",
    "    latex_code += \"\\\\end{tabular}\\n\\\\caption{Number of parameters per layer}\\n\\\\label{tab:parameters}\\n\\\\end{table}\\n\"\n",
    "    \n",
    "    return latex_code\n",
    "\n",
    "# Generate and print LaTeX code\n",
    "latex_output = generate_latex_table(model.model)\n",
    "print(latex_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{landscape}\n",
      "\\begin{table}[H]\n",
      "\\centering\n",
      "\\begin{tabular}{|c|c|}\n",
      "\\hline\n",
      "\\textbf{Layer Type} & \\textbf{Number of Parameters} \\\\\n",
      "\\hline\n",
      "LlamaModel (x1) & 128404224 \\\\\n",
      "\\hline\n",
      "Embedding (x1) & 24576000 \\\\\n",
      "\\hline\n",
      "ModuleList (x1) & 103827456 \\\\\n",
      "\\hline\n",
      "LlamaDecoderLayer (x12) & 103827456 \\\\\n",
      "\\hline\n",
      "LlamaSdpaAttention (x12) & 18874368 \\\\\n",
      "\\hline\n",
      "Linear (x84) & 103809024 \\\\\n",
      "\\hline\n",
      "LlamaMLP (x12) & 84934656 \\\\\n",
      "\\hline\n",
      "LlamaRMSNorm (x25) & 19200 \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\\end{landscape}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# Function to generate LaTeX table for model parameters\n",
    "def generate_latex_table(model):\n",
    "    layer_summary = defaultdict(lambda: {'count': 0, 'params': 0})\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        params = sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "        if params > 0:\n",
    "            layer_type = str(type(module)).split(\".\")[-1].split(\"'\")[0]\n",
    "            layer_summary[layer_type]['count'] += 1\n",
    "            layer_summary[layer_type]['params'] += params\n",
    "\n",
    "    # Begin LaTeX table code\n",
    "    latex_code = \"\\\\begin{landscape}\\n\"\n",
    "    latex_code += \"\\\\begin{table}[H]\\n\\\\centering\\n\\\\begin{tabular}{|c|c|}\\n\\\\hline\\n\"\n",
    "    latex_code += \"\\\\textbf{Layer Type} & \\\\textbf{Number of Parameters} \\\\\\\\\\n\\\\hline\\n\"\n",
    "\n",
    "    # Fill in table with layer information\n",
    "    for layer_type, info in layer_summary.items():\n",
    "        latex_code += f\"{layer_type} (x{info['count']}) & {info['params']} \\\\\\\\\\n\\\\hline\\n\"\n",
    "    \n",
    "    latex_code += \"\\\\end{tabular}\\n\\\\end{table}\\n\\\\end{landscape}\\n\"\n",
    "\n",
    "    return latex_code\n",
    "\n",
    "# Generate and print LaTeX code\n",
    "latex_output = generate_latex_table(model.model)\n",
    "print(latex_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}[H]\n",
      "\\centering\n",
      "\\begin{tabular}{|c|c|}\n",
      "\\hline\n",
      "Layer Type & Number of Parameters \\\\\n",
      "\\hline\n",
      "embedding & 24576000 \\\\\n",
      "attention & 18874368 \\\\\n",
      "mlp & 84934656 \\\\\n",
      "normalization & 19200 \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "\\caption{Grouped parameter counts by layer type}\n",
      "\\end{table}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def group_parameters_by_layer_type(parameter_details):\n",
    "    # Define categories for grouping\n",
    "    categories = {\n",
    "        'embedding': ['embed_tokens'],\n",
    "        'attention': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'self_attn'],\n",
    "        'mlp': ['gate_proj', 'up_proj', 'down_proj'],\n",
    "        'normalization': ['input_layernorm', 'post_attention_layernorm', 'norm'],\n",
    "        'output': ['lm_head']\n",
    "    }\n",
    "    \n",
    "    # Initialize the dictionary for grouped data\n",
    "    grouped_data = defaultdict(int)\n",
    "    \n",
    "    # Group data\n",
    "    for name, count in parameter_details:\n",
    "        for category, keywords in categories.items():\n",
    "            if any(key in name for key in keywords):\n",
    "                grouped_data[category] += count\n",
    "                break\n",
    "    \n",
    "    return grouped_data\n",
    "\n",
    "\n",
    "\n",
    "grouped_parameters = group_parameters_by_layer_type(parameter_details)\n",
    "\n",
    "# Generate LaTeX code\n",
    "def generate_latex_table(grouped_data):\n",
    "    latex = \"\\\\begin{table}[H]\\n\\\\centering\\n\\\\begin{tabular}{|c|c|}\\n\\\\hline\\n\"\n",
    "    latex += \"Layer Type & Number of Parameters \\\\\\\\\\n\\\\hline\\n\"\n",
    "    for category, count in grouped_data.items():\n",
    "        latex += f\"{category} & {count} \\\\\\\\\\n\"\n",
    "    latex += \"\\\\hline\\n\\\\end{tabular}\\n\\\\caption{Grouped parameter counts by layer type}\\n\\\\end{table}\"\n",
    "    return latex\n",
    "\n",
    "latex_code = generate_latex_table(grouped_parameters)\n",
    "print(latex_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}[H]\n",
      "\\centering\n",
      "\\begin{tabular}{|l|r|l|}\n",
      "\\hline\n",
      "Layer Name & Number of Parameters & Group \\\\\n",
      "\\hline\n",
      "Embedding & 24,576,000 & Embedding \\\\\n",
      "LlamaDecoderLayer 0 - self\\_attn.q\\_proj.weight & 589,824 & LlamaDecoderLayer 0 \\\\\n",
      "LlamaDecoderLayer 0 - self\\_attn.k\\_proj.weight & 196,608 & LlamaDecoderLayer 0 \\\\\n",
      "LlamaDecoderLayer 0 - self\\_attn.v\\_proj.weight & 196,608 & LlamaDecoderLayer 0 \\\\\n",
      "LlamaDecoderLayer 0 - self\\_attn.o\\_proj.weight & 589,824 & LlamaDecoderLayer 0 \\\\\n",
      "LlamaDecoderLayer 0 - mlp.gate\\_proj.weight & 2,359,296 & LlamaDecoderLayer 0 \\\\\n",
      "LlamaDecoderLayer 0 - mlp.up\\_proj.weight & 2,359,296 & LlamaDecoderLayer 0 \\\\\n",
      "LlamaDecoderLayer 0 - mlp.down\\_proj.weight & 2,359,296 & LlamaDecoderLayer 0 \\\\\n",
      "LlamaDecoderLayer 0 - input\\_layernorm.weight & 768 & LlamaDecoderLayer 0 \\\\\n",
      "LlamaDecoderLayer 0 - post\\_attention\\_layernorm.weight & 768 & LlamaDecoderLayer 0 \\\\\n",
      "LlamaDecoderLayer 1 - self\\_attn.q\\_proj.weight & 589,824 & LlamaDecoderLayer 1 \\\\\n",
      "LlamaDecoderLayer 1 - self\\_attn.k\\_proj.weight & 196,608 & LlamaDecoderLayer 1 \\\\\n",
      "LlamaDecoderLayer 1 - self\\_attn.v\\_proj.weight & 196,608 & LlamaDecoderLayer 1 \\\\\n",
      "LlamaDecoderLayer 1 - self\\_attn.o\\_proj.weight & 589,824 & LlamaDecoderLayer 1 \\\\\n",
      "LlamaDecoderLayer 1 - mlp.gate\\_proj.weight & 2,359,296 & LlamaDecoderLayer 1 \\\\\n",
      "LlamaDecoderLayer 1 - mlp.up\\_proj.weight & 2,359,296 & LlamaDecoderLayer 1 \\\\\n",
      "LlamaDecoderLayer 1 - mlp.down\\_proj.weight & 2,359,296 & LlamaDecoderLayer 1 \\\\\n",
      "LlamaDecoderLayer 1 - input\\_layernorm.weight & 768 & LlamaDecoderLayer 1 \\\\\n",
      "LlamaDecoderLayer 1 - post\\_attention\\_layernorm.weight & 768 & LlamaDecoderLayer 1 \\\\\n",
      "LlamaDecoderLayer 2 - self\\_attn.q\\_proj.weight & 589,824 & LlamaDecoderLayer 2 \\\\\n",
      "LlamaDecoderLayer 2 - self\\_attn.k\\_proj.weight & 196,608 & LlamaDecoderLayer 2 \\\\\n",
      "LlamaDecoderLayer 2 - self\\_attn.v\\_proj.weight & 196,608 & LlamaDecoderLayer 2 \\\\\n",
      "LlamaDecoderLayer 2 - self\\_attn.o\\_proj.weight & 589,824 & LlamaDecoderLayer 2 \\\\\n",
      "LlamaDecoderLayer 2 - mlp.gate\\_proj.weight & 2,359,296 & LlamaDecoderLayer 2 \\\\\n",
      "LlamaDecoderLayer 2 - mlp.up\\_proj.weight & 2,359,296 & LlamaDecoderLayer 2 \\\\\n",
      "LlamaDecoderLayer 2 - mlp.down\\_proj.weight & 2,359,296 & LlamaDecoderLayer 2 \\\\\n",
      "LlamaDecoderLayer 2 - input\\_layernorm.weight & 768 & LlamaDecoderLayer 2 \\\\\n",
      "LlamaDecoderLayer 2 - post\\_attention\\_layernorm.weight & 768 & LlamaDecoderLayer 2 \\\\\n",
      "LlamaDecoderLayer 3 - self\\_attn.q\\_proj.weight & 589,824 & LlamaDecoderLayer 3 \\\\\n",
      "LlamaDecoderLayer 3 - self\\_attn.k\\_proj.weight & 196,608 & LlamaDecoderLayer 3 \\\\\n",
      "LlamaDecoderLayer 3 - self\\_attn.v\\_proj.weight & 196,608 & LlamaDecoderLayer 3 \\\\\n",
      "LlamaDecoderLayer 3 - self\\_attn.o\\_proj.weight & 589,824 & LlamaDecoderLayer 3 \\\\\n",
      "LlamaDecoderLayer 3 - mlp.gate\\_proj.weight & 2,359,296 & LlamaDecoderLayer 3 \\\\\n",
      "LlamaDecoderLayer 3 - mlp.up\\_proj.weight & 2,359,296 & LlamaDecoderLayer 3 \\\\\n",
      "LlamaDecoderLayer 3 - mlp.down\\_proj.weight & 2,359,296 & LlamaDecoderLayer 3 \\\\\n",
      "LlamaDecoderLayer 3 - input\\_layernorm.weight & 768 & LlamaDecoderLayer 3 \\\\\n",
      "LlamaDecoderLayer 3 - post\\_attention\\_layernorm.weight & 768 & LlamaDecoderLayer 3 \\\\\n",
      "LlamaDecoderLayer 4 - self\\_attn.q\\_proj.weight & 589,824 & LlamaDecoderLayer 4 \\\\\n",
      "LlamaDecoderLayer 4 - self\\_attn.k\\_proj.weight & 196,608 & LlamaDecoderLayer 4 \\\\\n",
      "LlamaDecoderLayer 4 - self\\_attn.v\\_proj.weight & 196,608 & LlamaDecoderLayer 4 \\\\\n",
      "LlamaDecoderLayer 4 - self\\_attn.o\\_proj.weight & 589,824 & LlamaDecoderLayer 4 \\\\\n",
      "LlamaDecoderLayer 4 - mlp.gate\\_proj.weight & 2,359,296 & LlamaDecoderLayer 4 \\\\\n",
      "LlamaDecoderLayer 4 - mlp.up\\_proj.weight & 2,359,296 & LlamaDecoderLayer 4 \\\\\n",
      "LlamaDecoderLayer 4 - mlp.down\\_proj.weight & 2,359,296 & LlamaDecoderLayer 4 \\\\\n",
      "LlamaDecoderLayer 4 - input\\_layernorm.weight & 768 & LlamaDecoderLayer 4 \\\\\n",
      "LlamaDecoderLayer 4 - post\\_attention\\_layernorm.weight & 768 & LlamaDecoderLayer 4 \\\\\n",
      "LlamaDecoderLayer 5 - self\\_attn.q\\_proj.weight & 589,824 & LlamaDecoderLayer 5 \\\\\n",
      "LlamaDecoderLayer 5 - self\\_attn.k\\_proj.weight & 196,608 & LlamaDecoderLayer 5 \\\\\n",
      "LlamaDecoderLayer 5 - self\\_attn.v\\_proj.weight & 196,608 & LlamaDecoderLayer 5 \\\\\n",
      "LlamaDecoderLayer 5 - self\\_attn.o\\_proj.weight & 589,824 & LlamaDecoderLayer 5 \\\\\n",
      "LlamaDecoderLayer 5 - mlp.gate\\_proj.weight & 2,359,296 & LlamaDecoderLayer 5 \\\\\n",
      "LlamaDecoderLayer 5 - mlp.up\\_proj.weight & 2,359,296 & LlamaDecoderLayer 5 \\\\\n",
      "LlamaDecoderLayer 5 - mlp.down\\_proj.weight & 2,359,296 & LlamaDecoderLayer 5 \\\\\n",
      "LlamaDecoderLayer 5 - input\\_layernorm.weight & 768 & LlamaDecoderLayer 5 \\\\\n",
      "LlamaDecoderLayer 5 - post\\_attention\\_layernorm.weight & 768 & LlamaDecoderLayer 5 \\\\\n",
      "LlamaDecoderLayer 6 - self\\_attn.q\\_proj.weight & 589,824 & LlamaDecoderLayer 6 \\\\\n",
      "LlamaDecoderLayer 6 - self\\_attn.k\\_proj.weight & 196,608 & LlamaDecoderLayer 6 \\\\\n",
      "LlamaDecoderLayer 6 - self\\_attn.v\\_proj.weight & 196,608 & LlamaDecoderLayer 6 \\\\\n",
      "LlamaDecoderLayer 6 - self\\_attn.o\\_proj.weight & 589,824 & LlamaDecoderLayer 6 \\\\\n",
      "LlamaDecoderLayer 6 - mlp.gate\\_proj.weight & 2,359,296 & LlamaDecoderLayer 6 \\\\\n",
      "LlamaDecoderLayer 6 - mlp.up\\_proj.weight & 2,359,296 & LlamaDecoderLayer 6 \\\\\n",
      "LlamaDecoderLayer 6 - mlp.down\\_proj.weight & 2,359,296 & LlamaDecoderLayer 6 \\\\\n",
      "LlamaDecoderLayer 6 - input\\_layernorm.weight & 768 & LlamaDecoderLayer 6 \\\\\n",
      "LlamaDecoderLayer 6 - post\\_attention\\_layernorm.weight & 768 & LlamaDecoderLayer 6 \\\\\n",
      "LlamaDecoderLayer 7 - self\\_attn.q\\_proj.weight & 589,824 & LlamaDecoderLayer 7 \\\\\n",
      "LlamaDecoderLayer 7 - self\\_attn.k\\_proj.weight & 196,608 & LlamaDecoderLayer 7 \\\\\n",
      "LlamaDecoderLayer 7 - self\\_attn.v\\_proj.weight & 196,608 & LlamaDecoderLayer 7 \\\\\n",
      "LlamaDecoderLayer 7 - self\\_attn.o\\_proj.weight & 589,824 & LlamaDecoderLayer 7 \\\\\n",
      "LlamaDecoderLayer 7 - mlp.gate\\_proj.weight & 2,359,296 & LlamaDecoderLayer 7 \\\\\n",
      "LlamaDecoderLayer 7 - mlp.up\\_proj.weight & 2,359,296 & LlamaDecoderLayer 7 \\\\\n",
      "LlamaDecoderLayer 7 - mlp.down\\_proj.weight & 2,359,296 & LlamaDecoderLayer 7 \\\\\n",
      "LlamaDecoderLayer 7 - input\\_layernorm.weight & 768 & LlamaDecoderLayer 7 \\\\\n",
      "LlamaDecoderLayer 7 - post\\_attention\\_layernorm.weight & 768 & LlamaDecoderLayer 7 \\\\\n",
      "LlamaDecoderLayer 8 - self\\_attn.q\\_proj.weight & 589,824 & LlamaDecoderLayer 8 \\\\\n",
      "LlamaDecoderLayer 8 - self\\_attn.k\\_proj.weight & 196,608 & LlamaDecoderLayer 8 \\\\\n",
      "LlamaDecoderLayer 8 - self\\_attn.v\\_proj.weight & 196,608 & LlamaDecoderLayer 8 \\\\\n",
      "LlamaDecoderLayer 8 - self\\_attn.o\\_proj.weight & 589,824 & LlamaDecoderLayer 8 \\\\\n",
      "LlamaDecoderLayer 8 - mlp.gate\\_proj.weight & 2,359,296 & LlamaDecoderLayer 8 \\\\\n",
      "LlamaDecoderLayer 8 - mlp.up\\_proj.weight & 2,359,296 & LlamaDecoderLayer 8 \\\\\n",
      "LlamaDecoderLayer 8 - mlp.down\\_proj.weight & 2,359,296 & LlamaDecoderLayer 8 \\\\\n",
      "LlamaDecoderLayer 8 - input\\_layernorm.weight & 768 & LlamaDecoderLayer 8 \\\\\n",
      "LlamaDecoderLayer 8 - post\\_attention\\_layernorm.weight & 768 & LlamaDecoderLayer 8 \\\\\n",
      "LlamaDecoderLayer 9 - self\\_attn.q\\_proj.weight & 589,824 & LlamaDecoderLayer 9 \\\\\n",
      "LlamaDecoderLayer 9 - self\\_attn.k\\_proj.weight & 196,608 & LlamaDecoderLayer 9 \\\\\n",
      "LlamaDecoderLayer 9 - self\\_attn.v\\_proj.weight & 196,608 & LlamaDecoderLayer 9 \\\\\n",
      "LlamaDecoderLayer 9 - self\\_attn.o\\_proj.weight & 589,824 & LlamaDecoderLayer 9 \\\\\n",
      "LlamaDecoderLayer 9 - mlp.gate\\_proj.weight & 2,359,296 & LlamaDecoderLayer 9 \\\\\n",
      "LlamaDecoderLayer 9 - mlp.up\\_proj.weight & 2,359,296 & LlamaDecoderLayer 9 \\\\\n",
      "LlamaDecoderLayer 9 - mlp.down\\_proj.weight & 2,359,296 & LlamaDecoderLayer 9 \\\\\n",
      "LlamaDecoderLayer 9 - input\\_layernorm.weight & 768 & LlamaDecoderLayer 9 \\\\\n",
      "LlamaDecoderLayer 9 - post\\_attention\\_layernorm.weight & 768 & LlamaDecoderLayer 9 \\\\\n",
      "LlamaDecoderLayer 10 - self\\_attn.q\\_proj.weight & 589,824 & LlamaDecoderLayer 10 \\\\\n",
      "LlamaDecoderLayer 10 - self\\_attn.k\\_proj.weight & 196,608 & LlamaDecoderLayer 10 \\\\\n",
      "LlamaDecoderLayer 10 - self\\_attn.v\\_proj.weight & 196,608 & LlamaDecoderLayer 10 \\\\\n",
      "LlamaDecoderLayer 10 - self\\_attn.o\\_proj.weight & 589,824 & LlamaDecoderLayer 10 \\\\\n",
      "LlamaDecoderLayer 10 - mlp.gate\\_proj.weight & 2,359,296 & LlamaDecoderLayer 10 \\\\\n",
      "LlamaDecoderLayer 10 - mlp.up\\_proj.weight & 2,359,296 & LlamaDecoderLayer 10 \\\\\n",
      "LlamaDecoderLayer 10 - mlp.down\\_proj.weight & 2,359,296 & LlamaDecoderLayer 10 \\\\\n",
      "LlamaDecoderLayer 10 - input\\_layernorm.weight & 768 & LlamaDecoderLayer 10 \\\\\n",
      "LlamaDecoderLayer 10 - post\\_attention\\_layernorm.weight & 768 & LlamaDecoderLayer 10 \\\\\n",
      "LlamaDecoderLayer 11 - self\\_attn.q\\_proj.weight & 589,824 & LlamaDecoderLayer 11 \\\\\n",
      "LlamaDecoderLayer 11 - self\\_attn.k\\_proj.weight & 196,608 & LlamaDecoderLayer 11 \\\\\n",
      "LlamaDecoderLayer 11 - self\\_attn.v\\_proj.weight & 196,608 & LlamaDecoderLayer 11 \\\\\n",
      "LlamaDecoderLayer 11 - self\\_attn.o\\_proj.weight & 589,824 & LlamaDecoderLayer 11 \\\\\n",
      "LlamaDecoderLayer 11 - mlp.gate\\_proj.weight & 2,359,296 & LlamaDecoderLayer 11 \\\\\n",
      "LlamaDecoderLayer 11 - mlp.up\\_proj.weight & 2,359,296 & LlamaDecoderLayer 11 \\\\\n",
      "LlamaDecoderLayer 11 - mlp.down\\_proj.weight & 2,359,296 & LlamaDecoderLayer 11 \\\\\n",
      "LlamaDecoderLayer 11 - input\\_layernorm.weight & 768 & LlamaDecoderLayer 11 \\\\\n",
      "LlamaDecoderLayer 11 - post\\_attention\\_layernorm.weight & 768 & LlamaDecoderLayer 11 \\\\\n",
      "norm.weight & 768 & Miscellaneous \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "\\caption{Detailed parameter counts by layer and group}\n",
      "\\end{table}\n"
     ]
    }
   ],
   "source": [
    "def detailed_layer_info(parameter_details):\n",
    "    # Initialize dictionaries for grouping layers and their parameters\n",
    "    grouped_data = []\n",
    "    \n",
    "    # Process each parameter detail entry\n",
    "    for name, count in parameter_details:\n",
    "        # Simplify the layer naming by removing model prefix and handling sublayers\n",
    "        simplified_name = name.replace('model.', '')\n",
    "        parts = simplified_name.split('.')\n",
    "        if 'layers' in parts:\n",
    "            # Capture layer number and sub-layer detail\n",
    "            layer_number = parts[1]\n",
    "            sub_layer_name = '.'.join(parts[2:])\n",
    "            layer_label = f\"LlamaDecoderLayer {layer_number} - {sub_layer_name}\"\n",
    "            group = f\"LlamaDecoderLayer {layer_number}\"\n",
    "        elif 'embed_tokens' in parts:\n",
    "            layer_label = 'Embedding'\n",
    "            group = 'Embedding'\n",
    "        elif 'lm_head' in parts:\n",
    "            layer_label = 'Output Head'\n",
    "            group = 'Output Head'\n",
    "        else:\n",
    "            layer_label = simplified_name\n",
    "            group = 'Miscellaneous'\n",
    "        \n",
    "        # Append data with handling to ensure unique layer descriptions\n",
    "        grouped_data.append((layer_label, count, group))\n",
    "\n",
    "    return grouped_data\n",
    "\n",
    "\n",
    "grouped_parameters = detailed_layer_info(parameter_details)\n",
    "\n",
    "def generate_latex_table(grouped_parameters):\n",
    "    latex = \"\\\\begin{table}[H]\\n\\\\centering\\n\\\\begin{tabular}{|l|r|l|}\\n\\\\hline\\n\"\n",
    "    latex += \"Layer Name & Number of Parameters & Group \\\\\\\\\\n\\\\hline\\n\"\n",
    "    for layer_name, param_count, group in grouped_parameters:\n",
    "        # Ensure that underscores are properly escaped for LaTeX\n",
    "        safe_layer_name = layer_name.replace('_', '\\\\_')\n",
    "        latex += f\"{safe_layer_name} & {param_count:,} & {group} \\\\\\\\\\n\"\n",
    "    latex += \"\\\\hline\\n\\\\end{tabular}\\n\\\\caption{Detailed parameter counts by layer and group}\\n\\\\end{table}\"\n",
    "    return latex\n",
    "\n",
    "latex_code = generate_latex_table(grouped_parameters)\n",
    "print(latex_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "polygraf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
