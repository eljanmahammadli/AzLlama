{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import fasttext # language detection\n",
    "from huggingface_hub import hf_hub_download # download models from huggingface hub\n",
    "from nltk.tokenize import word_tokenize # tokenize words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API:\n",
    "- Filtering functions should return `True` or `False`\n",
    "    - Some will need threshold tuning, so some of them will return two functions, one of which will return number\n",
    "- Cleaning functions should take document as input and return cleaned document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'salam necəsən? Mən Azərbaycan dilində yazılmış bir proqramam.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def standardize_whitespace(document: str):\n",
    "    whitespaces = {\n",
    "        \" \",\n",
    "        \" \",\n",
    "        \" \",\n",
    "        \" \",\n",
    "        \" \",\n",
    "        \"　\",\n",
    "        \" \",\n",
    "        \" \",\n",
    "        \" \",\n",
    "        \" \",\n",
    "        \"￼\",\n",
    "        \"\",\n",
    "    }\n",
    "    document = \"\".join([char if char not in whitespaces else \" \" for char in document])\n",
    "    return document\n",
    "\n",
    "\n",
    "standardize_whitespace(\"salam necəsən? Mən Azərbaycan dilində yazılmış bir proqramam.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True, 1.0000059604644775)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### language filtering with `fasttext`\n",
    "# https://huggingface.co/facebook/fasttext-language-identification\n",
    "lang_detect_model_path = hf_hub_download(repo_id=\"facebook/fasttext-language-identification\", filename=\"model.bin\")\n",
    "model = fasttext.load_model(lang_detect_model_path)\n",
    "\n",
    "def detect_language(text):\n",
    "    \"\"\"Higher the better\"\"\"\n",
    "    pred = model.predict(text)\n",
    "    language = pred[0][0].replace(\"__label__\", \"\")\n",
    "    is_azerbaijani = True if language == \"azj_Latn\" else False\n",
    "    return is_azerbaijani, pred[1][0]\n",
    "\n",
    "# detect_language(\"salam necəsən? Mən Azərbaycan dilində yazılmış bir proqramam.\")\n",
    "detect_language(input(\"Enter a text: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['salam',\n",
       " 'necəsən',\n",
       " '?',\n",
       " 'Mən',\n",
       " 'Azərbaycan',\n",
       " 'dilində',\n",
       " 'yazılmış',\n",
       " 'bir',\n",
       " 'proqramam',\n",
       " '.',\n",
       " 'askmaksmn',\n",
       " 'dfeasfs']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtering documents on the number of words. Use `nltk.word_tokenize`.\n",
    "def get_word_count(document: str):\n",
    "    return len(word_tokenize(document))\n",
    "\n",
    "def filter_document_on_word_count(word_count, min_word_count, max_word_count):\n",
    "    if min_word_count <= word_count <= max_word_count:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "document = \"salam necəsən? Mən Azərbaycan dilində yazılmış bir proqramam.   askmaksmn\\ndfeasfs\"\n",
    "word_tokenize(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22857142857142856"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from math import sqrt\n",
    "\n",
    "def calculate_character_repetition_ratio(document, n=10, j=2):\n",
    "    n_grams = [document[i:i+n] for i in range(len(document) - n + 1)] # generate n-grams\n",
    "    n_gram_frequencies = Counter(n_grams) # count frequencies of n-grams\n",
    "    nn_grams = len(n_gram_frequencies) # calculate nn-grams\n",
    "    nrep_n_grams = int(sqrt(nn_grams)) if j == 2 else int(nn_grams**(1/j)) # calculate nrep-n-grams\n",
    "    # calculate character repetition ratio\n",
    "    most_common_nrep = n_gram_frequencies.most_common(nrep_n_grams)\n",
    "    sum_most_common_nrep = sum(freq for _, freq in most_common_nrep)\n",
    "    total_sum_n_grams = sum(n_gram_frequencies.values())\n",
    "    character_repetition_ratio = sum_most_common_nrep / total_sum_n_grams\n",
    "    return character_repetition_ratio\n",
    "\n",
    "# Example usage\n",
    "document = \"This is an example document to test the character repetition ratio.\"\n",
    "document = \"My name is Eljan and this document is written in English. And my name is Eljan hahah. And his name is Eljan hahah.\"\n",
    "calculate_character_repetition_ratio(document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"eljanmahammadli/azGPT-perplexity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁sal',\n",
       " 'am',\n",
       " '▁n',\n",
       " 'ec',\n",
       " 'əsən',\n",
       " '?',\n",
       " '▁Mən▁',\n",
       " 'Azərbaycan▁dilində▁',\n",
       " 'yazılmış',\n",
       " '▁bir▁',\n",
       " 'proqram',\n",
       " 'am',\n",
       " '.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"salam necəsən? Mən Azərbaycan dilində yazılmış bir proqramam.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'text', 'title'],\n",
       "    num_rows: 1213\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "ds = load_from_disk(\"/Users/eljan/Documents/azGPT/data/temp/az\")\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>696123</td>\n",
       "      <td>Maşa - obyektləri qavrayıb yerini dəyişdirmək ...</td>\n",
       "      <td>Maşa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>255121</td>\n",
       "      <td>Kəlbəli xan Əhməd xan oğlu Xoyski Rus ordusund...</td>\n",
       "      <td>Kəlbəli xan Xoyski</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>506863</td>\n",
       "      <td>Depersonalizasiya psixoloji xəstəlik növü. Bu ...</td>\n",
       "      <td>Depersonalizasiya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>813924</td>\n",
       "      <td>Seek and Destroy Takara Co Ltd. tərəfindən bur...</td>\n",
       "      <td>Seek and Destroy (2002 video oyun)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>279201</td>\n",
       "      <td>Merilin Cess Fransa pornoaktrisası. 70-dən çox...</td>\n",
       "      <td>Merilin Cess</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1208</th>\n",
       "      <td>181631</td>\n",
       "      <td>Üç yüz altı say sistemində ədədlərdən biridir....</td>\n",
       "      <td>306 (ədəd)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209</th>\n",
       "      <td>358827</td>\n",
       "      <td>Vanna Buakeva İqtisadçının şərəfini qoruyan tə...</td>\n",
       "      <td>Vanna Buakeva</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1210</th>\n",
       "      <td>436735</td>\n",
       "      <td>Fredrik Bayer Danimarka yazıçısı, müəllim və p...</td>\n",
       "      <td>Fredrik Bayer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1211</th>\n",
       "      <td>709421</td>\n",
       "      <td>Danimarka Vest-Hind şirkəti, və ya Danimarka V...</td>\n",
       "      <td>Vest-Hind şirkəti (Danimarka)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1212</th>\n",
       "      <td>797791</td>\n",
       "      <td>Qeyri-müəyyənlik cümlənin, müddəanın və ya həl...</td>\n",
       "      <td>Qeyri-müəyyənlik</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1213 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  ...                               title\n",
       "0     696123  ...                                Maşa\n",
       "1     255121  ...                  Kəlbəli xan Xoyski\n",
       "2     506863  ...                   Depersonalizasiya\n",
       "3     813924  ...  Seek and Destroy (2002 video oyun)\n",
       "4     279201  ...                        Merilin Cess\n",
       "...      ...  ...                                 ...\n",
       "1208  181631  ...                          306 (ədəd)\n",
       "1209  358827  ...                       Vanna Buakeva\n",
       "1210  436735  ...                       Fredrik Bayer\n",
       "1211  709421  ...       Vest-Hind şirkəti (Danimarka)\n",
       "1212  797791  ...                    Qeyri-müəyyənlik\n",
       "\n",
       "[1213 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "polygraf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
