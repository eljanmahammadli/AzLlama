{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby('data')\n",
    "\n",
    "idx = 0\n",
    "for book, group in tqdm(grouped):\n",
    "    # Concatenate all sentences into a single string for each book\n",
    "    full_text = ' '.join(group[' Sentence'])\n",
    "    \n",
    "    # Define the filename, using the book name. You might want to adjust this for valid filenames.\n",
    "    filename = f\"texts2/{idx}.txt\"\n",
    "    \n",
    "    # Write the concatenated text to a file\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        file.write(full_text)\n",
    "        \n",
    "    print(f\"Written '{filename}' with {len(full_text.split())} words.\")\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Ensure NLTK is set up with the necessary components\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def chunk_text_into_blocks(text, max_words=400, overlap=True):\n",
    "    sentences = sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_word_count = 0\n",
    "\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        words = sentence.split()\n",
    "        current_word_count += len(words)\n",
    "        current_chunk.append(sentence)\n",
    "        \n",
    "        # If the current word count exceeds the limit or it's the last sentence\n",
    "        if current_word_count >= max_words or i == len(sentences) - 1:\n",
    "            # Add the current chunk to the list of chunks\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = [sentence] if overlap else []  # Start new chunk with overlap if enabled\n",
    "            current_word_count = len(words)  # Reset word count\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Assuming your text files are named after the books and stored in a directory named 'books'\n",
    "book_files = [f for f in os.listdir('texts2') if f.endswith('.txt')]\n",
    "data = []\n",
    "\n",
    "for book_file in tqdm(book_files):\n",
    "    with open(f\"texts2/{book_file}\", 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    # Chunk the book's text\n",
    "    chunks = chunk_text_into_blocks(text, 400)\n",
    "    \n",
    "    # Add chunks to the data list, along with the book name for identification\n",
    "    book_name = book_file.replace('.txt', '').replace('_', ' ')\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        data.append({'book': book_name, 'chunk_id': i + 1, 'text': chunk})\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "df_chunks = pd.DataFrame(data)\n",
    "\n",
    "# Display the first few rows of the DataFrame to verify\n",
    "print(df_chunks.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kenlm_deepspeech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
