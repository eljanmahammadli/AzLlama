{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mistral-7b-instruct</th>\n",
       "      <th>llama-3-8b-instruct</th>\n",
       "      <th>mixtral-8x7b-instruct</th>\n",
       "      <th>mixtral-8x22b-instruct</th>\n",
       "      <th>llama3-70b</th>\n",
       "      <th>gemma-7b-it</th>\n",
       "      <th>gpt35-turbo</th>\n",
       "      <th>gpt4</th>\n",
       "      <th>llama2_70b</th>\n",
       "      <th>AzLlama-150M</th>\n",
       "      <th>claude_haiku</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Seni çözdüm, aşağıdaki yönerləmələr dəyərlidir...</td>\n",
       "      <td>Sevimli pizzanı hazırlamaq üçün, ilk növbədə, ...</td>\n",
       "      <td>Pizza, peşəkar tarzda hazırlanmaqla, pizza pət...</td>\n",
       "      <td>Pizza hazırlamaq üçün, şəxsi bir pizza doyğu (...</td>\n",
       "      <td>Pizza hazırlanması üçün lazım olan şeylər:\\n\\n...</td>\n",
       "      <td>Pizza hazırlanması üçün aşağıdakı adımlardan b...</td>\n",
       "      <td>Pizza hazırlamaq çox sadə və zövqverici bir pr...</td>\n",
       "      <td>Pizza hazırlamaq üçün aşağıdakı əsas addımları...</td>\n",
       "      <td>Pizza necə hazırlanır?\\n\\nPizza hazırlanırken...</td>\n",
       "      <td>Tərkibləri pizzanın üzərinə qoyaraq başlayın. ...</td>\n",
       "      <td>Pizza hazırlamaq üçün bir çox müxtəlif ingredi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Xoş gəlmisiniz! Sağlam qalmaq için önemli olma...</td>\n",
       "      <td>Sağlam qalmağın yolları çox vaxt fiziki fəaliy...</td>\n",
       "      <td>Məsali, sağlam qalmağın yollarıdır:\\n\\n1. Daha...</td>\n",
       "      <td>Sağlam qalmaq üçün əsası, dərmanlı tədbirlər, ...</td>\n",
       "      <td>Sağlamlıq üçün vacib olan yollar arasında aşağ...</td>\n",
       "      <td>Sağlam qalmağın yolları arasında:\\n\\n- Düzgün ...</td>\n",
       "      <td>Sağlam qalmaq və sağlamlıq üçün bir neçə əsas ...</td>\n",
       "      <td>Sağlam bir həyat tərzi qurmaq üçün aşağıdaki m...</td>\n",
       "      <td>Salam! Saglam qalmagin yollarinin neralaridir...</td>\n",
       "      <td>Sağlam bir qalmaq, xroniki xəstəliklərin qarşı...</td>\n",
       "      <td>Sağlam qalmağın yolları bir neçə əsas elementi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sen Kitab oxumağın faydaları haqqında sorduğun...</td>\n",
       "      <td>Kitab oxumağın çoxsaylı faydaları var. Birinci...</td>\n",
       "      <td>Xeyr, mən sizin sualınızı təşkil edə bilmirdim...</td>\n",
       "      <td>Kitab oxumağın faydaları çox sayıdır. Bu, fərd...</td>\n",
       "      <td>Kitab oxumağın çoxsaylı faydaları vardır. Aşağ...</td>\n",
       "      <td>Kitab oxumamanın faydaları çox sayıda olsa da,...</td>\n",
       "      <td>Kitab oxumaq insanın zehni və ruhi inkişafı üç...</td>\n",
       "      <td>Kitab oxumağın bir çox faydaları var, bu fayda...</td>\n",
       "      <td>Kitab oxumağın faydaları şu anda görülür:\\n\\n...</td>\n",
       "      <td>Kitabları oxumaq bir neçə səbəbdən uzun bir yo...</td>\n",
       "      <td>Kitab oxumağın çoxsaylı faydalı təsirləri var....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Senin vaxtını effektif bir şekilde idare etmək...</td>\n",
       "      <td>Vaxtını effektiv idarə etmək üçün bir neçə str...</td>\n",
       "      <td>Zəhmet olmasa, vaxtını idarə etmək üçün aşağıd...</td>\n",
       "      <td>Zamanı effektiv idarə etmək üçün, bir neçə əsa...</td>\n",
       "      <td>Vaxtını effektiv idarə etmək üçün aşağıdakı ad...</td>\n",
       "      <td>Vaxtın effektiv idarə edilmesi üçün aşağıdakil...</td>\n",
       "      <td>Vaxtı effektiv idarə etmək üçün bəzi əsas prin...</td>\n",
       "      <td>Vaxtınızı effektiv idarə etmək üçün aşağıdaki ...</td>\n",
       "      <td>Vaxtını effektiv idarə etmək üçün nə etmək la...</td>\n",
       "      <td>Səmərəliliyi artırmaq üçün tapşırıqların prior...</td>\n",
       "      <td>Vaxtın effektiv idarə edilməsi həyatımızın büt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Xoş gelmisiniz! Bu sorunun cəvdələxi üçün aşağ...</td>\n",
       "      <td>Yaxşı fotoşəkil çəkmək üçün bir neçə faktor di...</td>\n",
       "      <td>Məsələn, fotoşəklin qüdrələndirilməsi, kompozi...</td>\n",
       "      <td>Xoş gəlmisiniz! Fotoşəkil çəkmək üçün daha yax...</td>\n",
       "      <td>Yaxşı fotoşəkil çəkmək üçün diqqət etmək lazım...</td>\n",
       "      <td>Yaxşı fotoşəkil çəkmək üçün diqqət etmək gerek...</td>\n",
       "      <td>Yaxşı bir fotoşəkil çəkmək üçün bir neçə əsas ...</td>\n",
       "      <td>Yaxşı fotoşəkil çəkmək üçün bir neçə əsas aspe...</td>\n",
       "      <td>Hello! I'm happy to help you with your questi...</td>\n",
       "      <td>Yəqin ki, əla bir fotoqraf olmağınıza kömək ed...</td>\n",
       "      <td>Yaxşı fotoşəkil çəkmək üçün diqqət olunması la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Müasir incəsənət nümunələri, yani modern tekno...</td>\n",
       "      <td>Müasir incəsənət nümunələrinin cəmiyyətə təsir...</td>\n",
       "      <td>Müasir incəsənət nümunələrinin cəmiyyətə təsir...</td>\n",
       "      <td>Müasir incəsənət, cəmiyyətin dünya görüşünü, m...</td>\n",
       "      <td>Müasir incəsənət nümunələrinin cəmiyyətə təsir...</td>\n",
       "      <td>Müasir incəsənət nümunələri cəmiyyətə bir neçə...</td>\n",
       "      <td>Müasir incəsənət, cəmiyyətə çoxsa, mənsub oldu...</td>\n",
       "      <td>1. Yeni Fikirlərə Təşviq: Müasir incəsənət, hə...</td>\n",
       "      <td>Müasir incəsənət nümunələrinin cəmiyyətə təsi...</td>\n",
       "      <td>Keçmişdə geniş yayılmış və köklü olaraq görülə...</td>\n",
       "      <td>Müasir incəsənət nümunələrinin cəmiyyətə təsir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Xoş gelmisiniz! Şu sorusunuzda siz, sağlam qid...</td>\n",
       "      <td>Salam Sağlam qidalanmaq üçün ən yaxşı mətbəx a...</td>\n",
       "      <td>Məsələn, sağlam qidalanmaq üçün sizin işsizlik...</td>\n",
       "      <td>Müəlliflərə görə, sağlam qidalanmaq üçün ən ya...</td>\n",
       "      <td>Sizin sualınıza cavab verirəm!\\n\\nSağlam qidal...</td>\n",
       "      <td>Sağlam qidalanmaq üçün ən yaxşı mətbəx alətlər...</td>\n",
       "      <td>Sağlam qidalanmaq üçün mətbəxdə ən yaxşı alətl...</td>\n",
       "      <td>1. Blender: Sağlam smoothieler, suplar və sosl...</td>\n",
       "      <td>I am not able to provide a list of the three ...</td>\n",
       "      <td>Sağlam yemək seçməkdə və sağlam bir pəhrizin h...</td>\n",
       "      <td>Sağlam qidalanmaq üçün ən yaxşı mətbəx alətlər...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Senin sorgusunuzda, gələcək teknologiyaların İ...</td>\n",
       "      <td>Ziyarət edəniniz üçün rəhmət Gələcək texnologi...</td>\n",
       "      <td>Məsələn, gələcək texnologiyaların ictimai həya...</td>\n",
       "      <td>Gələcək texnologiyalar həmişə dəyişən və çox ş...</td>\n",
       "      <td>Gələcək texnologiyaların ictimai həyatımıza tə...</td>\n",
       "      <td>Gələcək texnologiyaların ictimai həyatımıza tə...</td>\n",
       "      <td>Gələcəkdə texnologiyalar ictimai həyatımıza də...</td>\n",
       "      <td>1. Uzaqdan İş: Gələcəkdə texnologiya, ən yaxşı...</td>\n",
       "      <td>Salam! Gələcək texnologiyaların ictimai həyat...</td>\n",
       "      <td>Gələcəyin proqnozlaşdırılması gələcəyimizi bir...</td>\n",
       "      <td>Gələcək texnologiyaların ictimai həyatımıza tə...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Senin sorguladığın zaman, \"Saatların tarixi və...</td>\n",
       "      <td>Sizin soruşduyunuz məsələyə cavab vermək üçün,...</td>\n",
       "      <td>Saatların tarixi ancient civilizationsda başla...</td>\n",
       "      <td>Saatların tarixi ilk əhvalatdan bərpa olaraq b...</td>\n",
       "      <td>Saatların tarixi çox uzun və maraqlıdır. Saatl...</td>\n",
       "      <td>Saatların tarixi əldəyə gedən uzun bir geçmişə...</td>\n",
       "      <td>Saatlar insanlar üçün ən əhəmiyyətli müxtəlifl...</td>\n",
       "      <td>1. Güneş saatları: İlk saatlar, güneşin kölgəs...</td>\n",
       "      <td>Saatların tarixi və onların inkişafı haqqında...</td>\n",
       "      <td>Saat dövrləri zamanla sürət, yer və ya vaxt də...</td>\n",
       "      <td>Saatların tarixi çox qədim dövrlərdən başlayır...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Xoş geldiniz! Biznes qurmaq üçün risklər hər z...</td>\n",
       "      <td>Əslən, yeni biznes qurmaq üçün risklər var, la...</td>\n",
       "      <td>Məsələn, yeni biznes qurmaq üçün şunları qəbul...</td>\n",
       "      <td>Xoş gəlmisiniz! Yeni biznes qurmaq üçün nəzərə...</td>\n",
       "      <td>Yeni bir biznes qurmaq zamanı nəzərə alınmalı ...</td>\n",
       "      <td>Yeni biznes qurmaq üçün nəzərə alınmalı risklə...</td>\n",
       "      <td>Yeni bir iş kurmak heyecan verici bir deneyim ...</td>\n",
       "      <td>1. Maliyyə Riskləri: Start-up biznesi qurarkən...</td>\n",
       "      <td>Yeni biznes qurmaq üçün nəzərə alınmalı riskl...</td>\n",
       "      <td>Yeni bir işə başlamaq bir neçə amildən təsirlə...</td>\n",
       "      <td>Yeni biznes qurmaq üçün nəzərə alınmalı əsas r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  mistral-7b-instruct  \\\n",
       "0   Seni çözdüm, aşağıdaki yönerləmələr dəyərlidir...   \n",
       "1   Xoş gəlmisiniz! Sağlam qalmaq için önemli olma...   \n",
       "2   Sen Kitab oxumağın faydaları haqqında sorduğun...   \n",
       "3   Senin vaxtını effektif bir şekilde idare etmək...   \n",
       "4   Xoş gelmisiniz! Bu sorunun cəvdələxi üçün aşağ...   \n",
       "..                                                ...   \n",
       "95  Müasir incəsənət nümunələri, yani modern tekno...   \n",
       "96  Xoş gelmisiniz! Şu sorusunuzda siz, sağlam qid...   \n",
       "97  Senin sorgusunuzda, gələcək teknologiyaların İ...   \n",
       "98  Senin sorguladığın zaman, \"Saatların tarixi və...   \n",
       "99  Xoş geldiniz! Biznes qurmaq üçün risklər hər z...   \n",
       "\n",
       "                                  llama-3-8b-instruct  \\\n",
       "0   Sevimli pizzanı hazırlamaq üçün, ilk növbədə, ...   \n",
       "1   Sağlam qalmağın yolları çox vaxt fiziki fəaliy...   \n",
       "2   Kitab oxumağın çoxsaylı faydaları var. Birinci...   \n",
       "3   Vaxtını effektiv idarə etmək üçün bir neçə str...   \n",
       "4   Yaxşı fotoşəkil çəkmək üçün bir neçə faktor di...   \n",
       "..                                                ...   \n",
       "95  Müasir incəsənət nümunələrinin cəmiyyətə təsir...   \n",
       "96  Salam Sağlam qidalanmaq üçün ən yaxşı mətbəx a...   \n",
       "97  Ziyarət edəniniz üçün rəhmət Gələcək texnologi...   \n",
       "98  Sizin soruşduyunuz məsələyə cavab vermək üçün,...   \n",
       "99  Əslən, yeni biznes qurmaq üçün risklər var, la...   \n",
       "\n",
       "                                mixtral-8x7b-instruct  \\\n",
       "0   Pizza, peşəkar tarzda hazırlanmaqla, pizza pət...   \n",
       "1   Məsali, sağlam qalmağın yollarıdır:\\n\\n1. Daha...   \n",
       "2   Xeyr, mən sizin sualınızı təşkil edə bilmirdim...   \n",
       "3   Zəhmet olmasa, vaxtını idarə etmək üçün aşağıd...   \n",
       "4   Məsələn, fotoşəklin qüdrələndirilməsi, kompozi...   \n",
       "..                                                ...   \n",
       "95  Müasir incəsənət nümunələrinin cəmiyyətə təsir...   \n",
       "96  Məsələn, sağlam qidalanmaq üçün sizin işsizlik...   \n",
       "97  Məsələn, gələcək texnologiyaların ictimai həya...   \n",
       "98  Saatların tarixi ancient civilizationsda başla...   \n",
       "99  Məsələn, yeni biznes qurmaq üçün şunları qəbul...   \n",
       "\n",
       "                               mixtral-8x22b-instruct  \\\n",
       "0   Pizza hazırlamaq üçün, şəxsi bir pizza doyğu (...   \n",
       "1   Sağlam qalmaq üçün əsası, dərmanlı tədbirlər, ...   \n",
       "2   Kitab oxumağın faydaları çox sayıdır. Bu, fərd...   \n",
       "3   Zamanı effektiv idarə etmək üçün, bir neçə əsa...   \n",
       "4   Xoş gəlmisiniz! Fotoşəkil çəkmək üçün daha yax...   \n",
       "..                                                ...   \n",
       "95  Müasir incəsənət, cəmiyyətin dünya görüşünü, m...   \n",
       "96  Müəlliflərə görə, sağlam qidalanmaq üçün ən ya...   \n",
       "97  Gələcək texnologiyalar həmişə dəyişən və çox ş...   \n",
       "98  Saatların tarixi ilk əhvalatdan bərpa olaraq b...   \n",
       "99  Xoş gəlmisiniz! Yeni biznes qurmaq üçün nəzərə...   \n",
       "\n",
       "                                           llama3-70b  \\\n",
       "0   Pizza hazırlanması üçün lazım olan şeylər:\\n\\n...   \n",
       "1   Sağlamlıq üçün vacib olan yollar arasında aşağ...   \n",
       "2   Kitab oxumağın çoxsaylı faydaları vardır. Aşağ...   \n",
       "3   Vaxtını effektiv idarə etmək üçün aşağıdakı ad...   \n",
       "4   Yaxşı fotoşəkil çəkmək üçün diqqət etmək lazım...   \n",
       "..                                                ...   \n",
       "95  Müasir incəsənət nümunələrinin cəmiyyətə təsir...   \n",
       "96  Sizin sualınıza cavab verirəm!\\n\\nSağlam qidal...   \n",
       "97  Gələcək texnologiyaların ictimai həyatımıza tə...   \n",
       "98  Saatların tarixi çox uzun və maraqlıdır. Saatl...   \n",
       "99  Yeni bir biznes qurmaq zamanı nəzərə alınmalı ...   \n",
       "\n",
       "                                          gemma-7b-it  \\\n",
       "0   Pizza hazırlanması üçün aşağıdakı adımlardan b...   \n",
       "1   Sağlam qalmağın yolları arasında:\\n\\n- Düzgün ...   \n",
       "2   Kitab oxumamanın faydaları çox sayıda olsa da,...   \n",
       "3   Vaxtın effektiv idarə edilmesi üçün aşağıdakil...   \n",
       "4   Yaxşı fotoşəkil çəkmək üçün diqqət etmək gerek...   \n",
       "..                                                ...   \n",
       "95  Müasir incəsənət nümunələri cəmiyyətə bir neçə...   \n",
       "96  Sağlam qidalanmaq üçün ən yaxşı mətbəx alətlər...   \n",
       "97  Gələcək texnologiyaların ictimai həyatımıza tə...   \n",
       "98  Saatların tarixi əldəyə gedən uzun bir geçmişə...   \n",
       "99  Yeni biznes qurmaq üçün nəzərə alınmalı risklə...   \n",
       "\n",
       "                                          gpt35-turbo  \\\n",
       "0   Pizza hazırlamaq çox sadə və zövqverici bir pr...   \n",
       "1   Sağlam qalmaq və sağlamlıq üçün bir neçə əsas ...   \n",
       "2   Kitab oxumaq insanın zehni və ruhi inkişafı üç...   \n",
       "3   Vaxtı effektiv idarə etmək üçün bəzi əsas prin...   \n",
       "4   Yaxşı bir fotoşəkil çəkmək üçün bir neçə əsas ...   \n",
       "..                                                ...   \n",
       "95  Müasir incəsənət, cəmiyyətə çoxsa, mənsub oldu...   \n",
       "96  Sağlam qidalanmaq üçün mətbəxdə ən yaxşı alətl...   \n",
       "97  Gələcəkdə texnologiyalar ictimai həyatımıza də...   \n",
       "98  Saatlar insanlar üçün ən əhəmiyyətli müxtəlifl...   \n",
       "99  Yeni bir iş kurmak heyecan verici bir deneyim ...   \n",
       "\n",
       "                                                 gpt4  \\\n",
       "0   Pizza hazırlamaq üçün aşağıdakı əsas addımları...   \n",
       "1   Sağlam bir həyat tərzi qurmaq üçün aşağıdaki m...   \n",
       "2   Kitab oxumağın bir çox faydaları var, bu fayda...   \n",
       "3   Vaxtınızı effektiv idarə etmək üçün aşağıdaki ...   \n",
       "4   Yaxşı fotoşəkil çəkmək üçün bir neçə əsas aspe...   \n",
       "..                                                ...   \n",
       "95  1. Yeni Fikirlərə Təşviq: Müasir incəsənət, hə...   \n",
       "96  1. Blender: Sağlam smoothieler, suplar və sosl...   \n",
       "97  1. Uzaqdan İş: Gələcəkdə texnologiya, ən yaxşı...   \n",
       "98  1. Güneş saatları: İlk saatlar, güneşin kölgəs...   \n",
       "99  1. Maliyyə Riskləri: Start-up biznesi qurarkən...   \n",
       "\n",
       "                                           llama2_70b  \\\n",
       "0    Pizza necə hazırlanır?\\n\\nPizza hazırlanırken...   \n",
       "1    Salam! Saglam qalmagin yollarinin neralaridir...   \n",
       "2    Kitab oxumağın faydaları şu anda görülür:\\n\\n...   \n",
       "3    Vaxtını effektiv idarə etmək üçün nə etmək la...   \n",
       "4    Hello! I'm happy to help you with your questi...   \n",
       "..                                                ...   \n",
       "95   Müasir incəsənət nümunələrinin cəmiyyətə təsi...   \n",
       "96   I am not able to provide a list of the three ...   \n",
       "97   Salam! Gələcək texnologiyaların ictimai həyat...   \n",
       "98   Saatların tarixi və onların inkişafı haqqında...   \n",
       "99   Yeni biznes qurmaq üçün nəzərə alınmalı riskl...   \n",
       "\n",
       "                                         AzLlama-150M  \\\n",
       "0   Tərkibləri pizzanın üzərinə qoyaraq başlayın. ...   \n",
       "1   Sağlam bir qalmaq, xroniki xəstəliklərin qarşı...   \n",
       "2   Kitabları oxumaq bir neçə səbəbdən uzun bir yo...   \n",
       "3   Səmərəliliyi artırmaq üçün tapşırıqların prior...   \n",
       "4   Yəqin ki, əla bir fotoqraf olmağınıza kömək ed...   \n",
       "..                                                ...   \n",
       "95  Keçmişdə geniş yayılmış və köklü olaraq görülə...   \n",
       "96  Sağlam yemək seçməkdə və sağlam bir pəhrizin h...   \n",
       "97  Gələcəyin proqnozlaşdırılması gələcəyimizi bir...   \n",
       "98  Saat dövrləri zamanla sürət, yer və ya vaxt də...   \n",
       "99  Yeni bir işə başlamaq bir neçə amildən təsirlə...   \n",
       "\n",
       "                                         claude_haiku  \n",
       "0   Pizza hazırlamaq üçün bir çox müxtəlif ingredi...  \n",
       "1   Sağlam qalmağın yolları bir neçə əsas elementi...  \n",
       "2   Kitab oxumağın çoxsaylı faydalı təsirləri var....  \n",
       "3   Vaxtın effektiv idarə edilməsi həyatımızın büt...  \n",
       "4   Yaxşı fotoşəkil çəkmək üçün diqqət olunması la...  \n",
       "..                                                ...  \n",
       "95  Müasir incəsənət nümunələrinin cəmiyyətə təsir...  \n",
       "96  Sağlam qidalanmaq üçün ən yaxşı mətbəx alətlər...  \n",
       "97  Gələcək texnologiyaların ictimai həyatımıza tə...  \n",
       "98  Saatların tarixi çox qədim dövrlərdən başlayır...  \n",
       "99  Yeni biznes qurmaq üçün nəzərə alınmalı əsas r...  \n",
       "\n",
       "[100 rows x 11 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('responses.csv')\n",
    "df.drop(columns=['prompt'], inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['mistral-7b-instruct', 'llama-3-8b-instruct', 'mixtral-8x7b-instruct',\n",
       "       'mixtral-8x22b-instruct', 'llama3-70b', 'gemma-7b-it', 'gpt35-turbo',\n",
       "       'gpt4', 'llama2_70b', 'AzLlama-150M', 'claude_haiku'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"eljanmahammadli/AzLlama-152M-Alpaca\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/eljan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/eljan/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/Users/eljan/anaconda3/envs/polygraf/lib/python3.11/site-packages/datasets/load.py:752: FutureWarning: The repository for sacrebleu contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.1/metrics/sacrebleu/sacrebleu.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/Users/eljan/anaconda3/envs/polygraf/lib/python3.11/site-packages/datasets/load.py:752: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.1/metrics/rouge/rouge.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/Users/eljan/anaconda3/envs/polygraf/lib/python3.11/site-packages/datasets/load.py:752: FutureWarning: The repository for meteor contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.1/metrics/meteor/meteor.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "[nltk_data] Downloading package wordnet to /Users/eljan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/eljan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/eljan/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Average BLEU  Average ROUGE-L  Average METEOR\n",
      "mistral-7b-instruct         2.398136         0.154001        0.135140\n",
      "llama-3-8b-instruct         3.462628         0.218719        0.160589\n",
      "mixtral-8x7b-instruct       2.772791         0.185411        0.158084\n",
      "mixtral-8x22b-instruct      3.310991         0.202824        0.163633\n",
      "llama3-70b                  5.451485         0.235059        0.192426\n",
      "gemma-7b-it                 2.894050         0.153424        0.141566\n",
      "gpt35-turbo                 5.602984         0.230242        0.208526\n",
      "gpt4                      100.000000         1.000000        1.000000\n",
      "llama2_70b                  2.777504         0.155017        0.121894\n",
      "AzLlama-150M                2.761985         0.185118        0.159104\n",
      "claude_haiku                6.640734         0.245525        0.227852\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_metric\n",
    "from transformers import AutoTokenizer\n",
    "import nltk\n",
    "\n",
    "# Ensure that NLTK has the necessary data for METEOR\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Load your custom tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"eljanmahammadli/AzLlama-152M-Alpaca\")\n",
    "\n",
    "# Example DataFrame with your model outputs (replace this with your actual data)\n",
    "# data = {\n",
    "#     'mistral-7b-instruct': [\"response1\", \"response2\"],\n",
    "#     'llama-3-8b-instruct': [\"response1\", \"response2\"],\n",
    "#     # Add other models' responses\n",
    "#     'gpt4': [\"ground truth1\", \"ground truth2\"]  # Ground truth\n",
    "# }\n",
    "\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# Metrics initialization\n",
    "bleu_metric = load_metric('sacrebleu')\n",
    "rouge_metric = load_metric('rouge')\n",
    "meteor_metric = load_metric('meteor')\n",
    "\n",
    "def calculate_scores(model_responses, reference):\n",
    "    scores = {\n",
    "        'BLEU': bleu_metric.compute(predictions=[model_responses], references=[[reference]])['score'],\n",
    "        'ROUGE-L': rouge_metric.compute(predictions=[model_responses], references=[reference])['rougeL'].mid.fmeasure,\n",
    "        'METEOR': meteor_metric.compute(predictions=[model_responses], references=[reference])['meteor']\n",
    "    }\n",
    "    return scores\n",
    "\n",
    "# Tokenize and compute scores\n",
    "results = {}\n",
    "for column in df.columns:\n",
    "    # if column != 'gpt4':  # Skip the ground truth column\n",
    "    tokenized_responses = [tokenizer.decode(tokenizer.encode(response)) for response in df[column]]\n",
    "    tokenized_truth = [tokenizer.decode(tokenizer.encode(truth)) for truth in df['gpt4']]\n",
    "    scores = [calculate_scores(resp, truth) for resp, truth in zip(tokenized_responses, tokenized_truth)]\n",
    "    results[column] = {\n",
    "        'Average BLEU': sum(score['BLEU'] for score in scores) / len(scores),\n",
    "        'Average ROUGE-L': sum(score['ROUGE-L'] for score in scores) / len(scores),\n",
    "        'Average METEOR': sum(score['METEOR'] for score in scores) / len(scores)\n",
    "    }\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results).transpose()\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                         Average BLEU  Average ROUGE-L  Average METEOR\n",
    "# mistral-7b-instruct         2.398136         0.154001        0.135140\n",
    "# llama-3-8b-instruct         3.462628         0.218719        0.160589\n",
    "# mixtral-8x7b-instruct       2.772791         0.185411        0.158084\n",
    "# mixtral-8x22b-instruct      3.310991         0.202824        0.163633\n",
    "# llama3-70b                  5.451485         0.235059        0.192426\n",
    "# gemma-7b-it                 2.894050         0.153424        0.141566\n",
    "# gpt35-turbo                 5.602984         0.230242        0.208526\n",
    "# gpt4                      100.000000         1.000000        1.000000\n",
    "# llama2_70b                  2.777504         0.155017        0.121894\n",
    "# AzLlama-150M                2.761985         0.185118        0.159104\n",
    "# claude_haiku                6.640734         0.245525        0.227852"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['mistral-7b-instruct', 'llama-3-8b-instruct', 'mixtral-8x7b-instruct', 'mixtral-8x22b-instruct', 'llama3-70b', 'gemma-7b-it', 'gpt35-turbo', 'gpt4', 'llama2_70b', 'AzLlama-150M', 'claude_haiku'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = df.sample(1, random_state=42).to_dict()\n",
    "d.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, references = [], []\n",
    "\n",
    "predictions.append(tokenizer.tokenize(d['gpt35-turbo'][83]))\n",
    "references.append(tokenizer.tokenize(d['gpt4'][83]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/eljan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            bleu   rouge-l\n",
      "mistral-7b-instruct     0.001056  0.036103\n",
      "llama-3-8b-instruct     0.001235  0.034499\n",
      "mixtral-8x7b-instruct   0.001251  0.031445\n",
      "mixtral-8x22b-instruct  0.001372  0.054883\n",
      "llama3-70b              0.001463  0.065928\n",
      "gemma-7b-it              0.00206  0.019085\n",
      "gpt35-turbo             0.001212  0.024563\n",
      "gpt4                         1.0       1.0\n",
      "llama2_70b              0.001211  0.073966\n",
      "AzLlama-150M            0.001661  0.017463\n",
      "claude_haiku            0.001216  0.045089\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge import Rouge\n",
    "\n",
    "# Load the data\n",
    "\n",
    "# Set the ground truth column\n",
    "ground_truth_col = 'gpt4'\n",
    "\n",
    "# Define the Azerbaijani language tokenizer\n",
    "nltk.download('punkt')\n",
    "tokenizer = tokenizer.tokenize\n",
    "\n",
    "# Define the evaluation metrics\n",
    "metrics = ['bleu', 'rouge-l']\n",
    "\n",
    "# Initialize the results dataframe\n",
    "results = pd.DataFrame(index=df.columns, columns=metrics)\n",
    "\n",
    "# Calculate the scores for each LLM\n",
    "for col in df.columns:\n",
    "    if col!= ground_truth_col:\n",
    "        # Tokenize the texts\n",
    "        refs = [tokenizer(text) for text in df[ground_truth_col]]\n",
    "        hyps = [tokenizer(text) for text in df[col]]\n",
    "        \n",
    "        # Calculate BLEU score\n",
    "        bleu_scores = [sentence_bleu(ref, hyp, smoothing_function=SmoothingFunction().method1) for ref, hyp in zip(refs, hyps)]\n",
    "        avg_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "        \n",
    "        # Calculate ROUGE-L score\n",
    "        rouge = Rouge()\n",
    "        rouge_scores = [rouge.get_scores(' '.join(hyp),''.join(ref))[0]['rouge-l']['f'] for ref, hyp in zip(refs, hyps)]\n",
    "        avg_rouge_l = sum(rouge_scores) / len(rouge_scores)\n",
    "        \n",
    "        # Store the results\n",
    "        results.loc[col, 'bleu'] = avg_bleu\n",
    "        results.loc[col, 'rouge-l'] = avg_rouge_l\n",
    "\n",
    "# Add the gpt4 scores (assuming it's the ground truth)\n",
    "results.loc[ground_truth_col, 'bleu'] = 1.0\n",
    "results.loc[ground_truth_col, 'rouge-l'] = 1.0\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word tokenize\n",
    "#                             bleu   rouge-l\n",
    "# mistral-7b-instruct     0.001316  0.051503\n",
    "# llama-3-8b-instruct     0.001287   0.03947\n",
    "# mixtral-8x7b-instruct   0.001377  0.039931\n",
    "# mixtral-8x22b-instruct  0.001558  0.064012\n",
    "# llama3-70b              0.001776  0.082354\n",
    "# gemma-7b-it             0.002643  0.026411\n",
    "# gpt35-turbo              0.00127  0.028687\n",
    "# gpt4                         1.0       1.0\n",
    "# llama2_70b              0.001464  0.085285\n",
    "# AzLlama-150M            0.001798  0.019751\n",
    "# claude_haiku            0.001386  0.053894"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eljan/anaconda3/envs/polygraf/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/eljan/anaconda3/envs/polygraf/lib/python3.11/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Model  Average BLEU  Average ROUGE-L  Average METEOR\n",
      "0     mistral-7b-instruct      0.085399         0.117270             NaN\n",
      "1     llama-3-8b-instruct      0.145927         0.159171             NaN\n",
      "2   mixtral-8x7b-instruct      0.113248         0.127245             NaN\n",
      "3  mixtral-8x22b-instruct      0.120173         0.152009             NaN\n",
      "4              llama3-70b      0.218366         0.217277             NaN\n",
      "5             gemma-7b-it      0.099347         0.122169             NaN\n",
      "6             gpt35-turbo      0.205948         0.188514             NaN\n",
      "7                    gpt4      0.210306         0.215233             NaN\n",
      "8              llama2_70b      0.086213         0.114292             NaN\n",
      "9            AzLlama-150M      0.106846         0.126251             NaN\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from rouge import Rouge\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"eljanmahammadli/AzLlama-152M-Alpaca\")\n",
    "\n",
    "# Function to tokenize text\n",
    "def tokenize_text(text):\n",
    "    return tokenizer.tokenize(text)\n",
    "\n",
    "# Calculate the scores\n",
    "def calculate_scores(df):\n",
    "    rouge = Rouge()\n",
    "    scores = {\n",
    "        'Model': [],\n",
    "        'Average BLEU': [],\n",
    "        'Average ROUGE-L': [],\n",
    "        'Average METEOR': []\n",
    "    }\n",
    "    \n",
    "    reference_texts = df['claude_haiku'].apply(tokenize_text).tolist()\n",
    "    models = df.columns[df.columns != 'claude_haiku']\n",
    "    \n",
    "    for model in models:\n",
    "        bleu_scores = []\n",
    "        rouge_l_scores = []\n",
    "        meteor_scores = []\n",
    "        \n",
    "        model_texts = df[model].apply(tokenize_text).tolist()\n",
    "        \n",
    "        for ref_text, model_text in zip(reference_texts, model_texts):\n",
    "            # Calculating BLEU\n",
    "            if len(ref_text) > 0 and len(model_text) > 0:  # BLEU requires non-empty sequences\n",
    "                bleu_scores.append(sentence_bleu([ref_text], model_text, weights = (1.0, 0, 0, 0), smoothing_function=SmoothingFunction().method1))\n",
    "                # meteor_scores.append(meteor_score([' '.join(ref_text)], ' '.join(model_text)))\n",
    "            \n",
    "            # Calculating ROUGE-L\n",
    "            rouge_score = rouge.get_scores(' '.join(model_text), ' '.join(ref_text))\n",
    "            rouge_l_scores.append(rouge_score[0]['rouge-l']['f'])\n",
    "        \n",
    "        scores['Model'].append(model)\n",
    "        scores['Average BLEU'].append(np.mean(bleu_scores))\n",
    "        scores['Average ROUGE-L'].append(np.mean(rouge_l_scores))\n",
    "        scores['Average METEOR'].append(np.mean(meteor_scores))\n",
    "\n",
    "    return pd.DataFrame(scores)\n",
    "\n",
    "# Calculate and print results\n",
    "results_df = calculate_scores(df)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref_text = \n",
    "# model_text\n",
    "\n",
    "predictions = ['It', 'is', 'to', 'insure', 'the', 'troops',\n",
    "               'forever', 'hearing', 'the', 'activity', 'guidebook',\n",
    "               'that', 'party', 'direct']\n",
    "reference = ['It', 'is', 'a', 'guide', 'to', 'action', 'that',\n",
    "               'ensures', 'that', 'the', 'military', 'will', 'forever',\n",
    "               'heed', 'Party', 'commands']\n",
    "\n",
    "# sentence_bleu([reference1], pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eljan/anaconda3/envs/polygraf/lib/python3.11/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/Users/eljan/anaconda3/envs/polygraf/lib/python3.11/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.37151909989293497"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "# Function to calculate BLEU with only 1-gram\n",
    "def calculate_bleu_1gram(reference, candidate):\n",
    "    # Configure BLEU to use only 1-gram\n",
    "    weights = (1.0, 0, 0, 0)  # This focuses entirely on 1-gram\n",
    "    return sentence_bleu([reference], candidate, weights=weights, smoothing_function=SmoothingFunction().method1)\n",
    "calculate_bleu_1gram(reference, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['gpt35-turbo'].tolist()), len(df['gpt4'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['mistral-7b-instruct', 'llama-3-8b-instruct', 'mixtral-8x7b-instruct',\n",
       "       'mixtral-8x22b-instruct', 'llama3-70b', 'gemma-7b-it', 'gpt35-turbo',\n",
       "       'gpt4', 'llama2_70b', 'AzLlama-150M', 'claude_haiku'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8602dfad36d4effb6548e76f33cb81f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e73aef43f2444ab3b534421e41f9c6a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 1/11 [00:29<04:59, 29.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 29.07 seconds, 3.44 sentences/sec\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caa2a80062db411dbbdc9aeb92548f27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 1/11 [00:48<08:02, 48.28s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results_df\n\u001b[1;32m     31\u001b[0m target_llm \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt4\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 32\u001b[0m scores_df \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_bertscore_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_llm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(scores_df)\n",
      "Cell \u001b[0;32mIn[42], line 20\u001b[0m, in \u001b[0;36mcalculate_bertscore_all\u001b[0;34m(df, target_column)\u001b[0m\n\u001b[1;32m     18\u001b[0m other_lm_predictions \u001b[38;5;241m=\u001b[39m df[column]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Calculate BERTScore\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m P, R, F1 \u001b[38;5;241m=\u001b[39m \u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_predictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother_lm_predictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maz\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Calculate overall scores by taking the mean\u001b[39;00m\n\u001b[1;32m     22\u001b[0m results[column] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAverage Precision\u001b[39m\u001b[38;5;124m'\u001b[39m: P\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem(),  \u001b[38;5;66;03m# Convert to Python float for readability\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAverage Recall\u001b[39m\u001b[38;5;124m'\u001b[39m: R\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAverage F1\u001b[39m\u001b[38;5;124m'\u001b[39m: F1\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     26\u001b[0m }\n",
      "File \u001b[0;32m~/anaconda3/envs/polygraf/lib/python3.11/site-packages/bert_score/score.py:123\u001b[0m, in \u001b[0;36mscore\u001b[0;34m(cands, refs, model_type, num_layers, verbose, idf, device, batch_size, nthreads, all_layers, lang, return_hash, rescale_with_baseline, baseline_path, use_fast_tokenizer)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalculating scores...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    122\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[0;32m--> 123\u001b[0m all_preds \u001b[38;5;241m=\u001b[39m \u001b[43mbert_cos_score_idf\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrefs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43midf_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mall_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ref_group_boundaries \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m     max_preds \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/polygraf/lib/python3.11/site-packages/bert_score/utils.py:616\u001b[0m, in \u001b[0;36mbert_cos_score_idf\u001b[0;34m(model, refs, hyps, tokenizer, idf_dict, verbose, batch_size, device, all_layers)\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_start \u001b[38;5;129;01min\u001b[39;00m iter_range:\n\u001b[1;32m    615\u001b[0m     sen_batch \u001b[38;5;241m=\u001b[39m sentences[batch_start : batch_start \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[0;32m--> 616\u001b[0m     embs, masks, padded_idf \u001b[38;5;241m=\u001b[39m \u001b[43mget_bert_embedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43msen_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midf_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_layers\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    619\u001b[0m     embs \u001b[38;5;241m=\u001b[39m embs\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m    620\u001b[0m     masks \u001b[38;5;241m=\u001b[39m masks\u001b[38;5;241m.\u001b[39mcpu()\n",
      "File \u001b[0;32m~/anaconda3/envs/polygraf/lib/python3.11/site-packages/bert_score/utils.py:455\u001b[0m, in \u001b[0;36mget_bert_embedding\u001b[0;34m(all_sens, model, tokenizer, idf_dict, batch_size, device, all_layers)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(all_sens), batch_size):\n\u001b[0;32m--> 455\u001b[0m         batch_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mbert_encode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpadded_sens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m            \u001b[49m\u001b[43mall_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    461\u001b[0m         embeddings\u001b[38;5;241m.\u001b[39mappend(batch_embedding)\n\u001b[1;32m    462\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m batch_embedding\n",
      "File \u001b[0;32m~/anaconda3/envs/polygraf/lib/python3.11/site-packages/bert_score/utils.py:351\u001b[0m, in \u001b[0;36mbert_encode\u001b[0;34m(model, x, attention_mask, all_layers)\u001b[0m\n\u001b[1;32m    349\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    350\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 351\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_layers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m all_layers:\n\u001b[1;32m    353\u001b[0m     emb \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(out[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/polygraf/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/polygraf/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/polygraf/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:1013\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1004\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1006\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1007\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1008\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1012\u001b[0m )\n\u001b[0;32m-> 1013\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1025\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1026\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/polygraf/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/polygraf/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/polygraf/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:607\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    596\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    597\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    598\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    604\u001b[0m         output_attentions,\n\u001b[1;32m    605\u001b[0m     )\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 607\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    617\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/anaconda3/envs/polygraf/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/polygraf/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/polygraf/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:497\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    487\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    494\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    496\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/polygraf/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/polygraf/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/polygraf/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:427\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    419\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    425\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    426\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 427\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    437\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/polygraf/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/polygraf/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/polygraf/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:365\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    363\u001b[0m     attention_probs \u001b[38;5;241m=\u001b[39m attention_probs \u001b[38;5;241m*\u001b[39m head_mask\n\u001b[0;32m--> 365\u001b[0m context_layer \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_layer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    367\u001b[0m context_layer \u001b[38;5;241m=\u001b[39m context_layer\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    368\u001b[0m new_context_layer_shape \u001b[38;5;241m=\u001b[39m context_layer\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_head_size,)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from bert_score import score\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"bert-base-multilingual-cased\"\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Function to calculate BERTScore for a target LM against all other LMs\n",
    "def calculate_bertscore_all(df, target_column):\n",
    "    target_predictions = df[target_column].tolist()\n",
    "    results = {}\n",
    "    \n",
    "    for column in tqdm(df.columns):\n",
    "        if column != target_column:  # Exclude the target LM from comparison against itself\n",
    "            other_lm_predictions = df[column].tolist()\n",
    "            # Calculate BERTScore\n",
    "            P, R, F1 = score(target_predictions, other_lm_predictions, model_type=model_name, lang=\"az\", verbose=True)\n",
    "            # Calculate overall scores by taking the mean\n",
    "            results[column] = {\n",
    "                'Average Precision': P.mean().item(),  # Convert to Python float for readability\n",
    "                'Average Recall': R.mean().item(),\n",
    "                'Average F1': F1.mean().item()\n",
    "            }\n",
    "            # convert to pandas df\n",
    "            results_df = pd.DataFrame(results).transpose()\n",
    "    return results_df\n",
    "\n",
    "target_llm = 'gpt4'\n",
    "scores_df = calculate_bertscore_all(df, target_llm)\n",
    "# sort by average F1 score\n",
    "scores_df = scores_df.sort_values('Average F1', ascending=False)\n",
    "print(scores_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6900662779808044, 0.6778497099876404, 0.6834918856620789)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate overall scores for precision, recall and F1\n",
    "P.mean().item(), R.mean().item(), F1.mean().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt35: (0.7256407141685486, 0.713125467300415, 0.7191633582115173)\n",
    "# mistral: (0.6443787217140198, 0.6333656907081604, 0.6386900544166565)\n",
    "# azllama: (0.6900662779808044, 0.6778497099876404, 0.6834918856620789)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Average BLEU</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>claude_haiku</td>\n",
       "      <td>42.729768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gpt35-turbo</td>\n",
       "      <td>41.058283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>llama3-70b</td>\n",
       "      <td>40.486636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AzLlama-150M</td>\n",
       "      <td>34.823319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>llama-3-8b-instruct</td>\n",
       "      <td>33.291946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mixtral-8x22b-instruct</td>\n",
       "      <td>33.045478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mixtral-8x7b-instruct</td>\n",
       "      <td>31.505338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gemma-7b-it</td>\n",
       "      <td>31.465663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mistral-7b-instruct</td>\n",
       "      <td>26.102194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>llama2_70b</td>\n",
       "      <td>23.357804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Model  Average BLEU\n",
       "9            claude_haiku     42.729768\n",
       "6             gpt35-turbo     41.058283\n",
       "4              llama3-70b     40.486636\n",
       "8            AzLlama-150M     34.823319\n",
       "1     llama-3-8b-instruct     33.291946\n",
       "3  mixtral-8x22b-instruct     33.045478\n",
       "2   mixtral-8x7b-instruct     31.505338\n",
       "5             gemma-7b-it     31.465663\n",
       "0     mistral-7b-instruct     26.102194\n",
       "7              llama2_70b     23.357804"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'mistral-7b-instruct': 26.102194238583426, 'llama-3-8b-instruct': 33.29194637506054, 'mixtral-8x7b-instruct': 31.50533847998187, 'mixtral-8x22b-instruct': 33.04547764011548, 'llama3-70b': 40.486635584978465, 'gemma-7b-it': 31.46566334341594, 'gpt35-turbo': 41.058283184197805, 'llama2_70b': 23.35780390859043, 'AzLlama-150M': 34.82331885067274, 'claude_haiku': 42.729768306950184}\n",
    "\n",
    "# convert to df and sort by scores\n",
    "scores_df = pd.DataFrame(d.items(), columns=['Model', 'Average BLEU']).sort_values('Average BLEU', ascending=False)\n",
    "scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Average Precision</th>\n",
       "      <th>Average Recall</th>\n",
       "      <th>Average F1</th>\n",
       "      <th># Parameters</th>\n",
       "      <th># Training Tokens</th>\n",
       "      <th>Access Type</th>\n",
       "      <th>semantic_scores</th>\n",
       "      <th>chrf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>claude_haiku</td>\n",
       "      <td>73.02</td>\n",
       "      <td>74.00</td>\n",
       "      <td>73.50</td>\n",
       "      <td>&gt;1T</td>\n",
       "      <td>No Information</td>\n",
       "      <td>proprietary</td>\n",
       "      <td>85.14</td>\n",
       "      <td>42.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt35-turbo</td>\n",
       "      <td>71.31</td>\n",
       "      <td>72.56</td>\n",
       "      <td>71.92</td>\n",
       "      <td>&gt;100B</td>\n",
       "      <td>\"No Information\"</td>\n",
       "      <td>proprietary</td>\n",
       "      <td>84.96</td>\n",
       "      <td>41.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>llama3-70b</td>\n",
       "      <td>68.94</td>\n",
       "      <td>72.58</td>\n",
       "      <td>70.69</td>\n",
       "      <td>70B</td>\n",
       "      <td>15T</td>\n",
       "      <td>open-weight</td>\n",
       "      <td>81.55</td>\n",
       "      <td>40.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AzLlama-150M</td>\n",
       "      <td>67.78</td>\n",
       "      <td>69.01</td>\n",
       "      <td>68.35</td>\n",
       "      <td>150M</td>\n",
       "      <td>9B</td>\n",
       "      <td>open-weight</td>\n",
       "      <td>82.87</td>\n",
       "      <td>34.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mixtral-8x22b-instruct</td>\n",
       "      <td>65.72</td>\n",
       "      <td>68.93</td>\n",
       "      <td>67.26</td>\n",
       "      <td>8x22B</td>\n",
       "      <td>No Information</td>\n",
       "      <td>open-weight</td>\n",
       "      <td>82.49</td>\n",
       "      <td>33.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>llama-3-8b-instruct</td>\n",
       "      <td>64.50</td>\n",
       "      <td>69.15</td>\n",
       "      <td>66.72</td>\n",
       "      <td>8B</td>\n",
       "      <td>15T</td>\n",
       "      <td>open-weight</td>\n",
       "      <td>82.30</td>\n",
       "      <td>33.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gemma-7b-it</td>\n",
       "      <td>64.75</td>\n",
       "      <td>68.78</td>\n",
       "      <td>66.66</td>\n",
       "      <td>7B</td>\n",
       "      <td>6T</td>\n",
       "      <td>open-weight</td>\n",
       "      <td>60.03</td>\n",
       "      <td>31.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mixtral-8x7b-instruct</td>\n",
       "      <td>65.52</td>\n",
       "      <td>66.89</td>\n",
       "      <td>66.18</td>\n",
       "      <td>8x7B</td>\n",
       "      <td>No Information</td>\n",
       "      <td>open-weight</td>\n",
       "      <td>75.60</td>\n",
       "      <td>31.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mistral-7b-instruct</td>\n",
       "      <td>63.34</td>\n",
       "      <td>64.44</td>\n",
       "      <td>63.87</td>\n",
       "      <td>7B</td>\n",
       "      <td>8T</td>\n",
       "      <td>open-weight</td>\n",
       "      <td>59.57</td>\n",
       "      <td>26.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>llama2_70b</td>\n",
       "      <td>59.08</td>\n",
       "      <td>63.40</td>\n",
       "      <td>61.13</td>\n",
       "      <td>70B</td>\n",
       "      <td>2T</td>\n",
       "      <td>open-weight</td>\n",
       "      <td>65.78</td>\n",
       "      <td>23.36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Model  Average Precision  Average Recall  Average F1  \\\n",
       "0            claude_haiku              73.02           74.00       73.50   \n",
       "1             gpt35-turbo              71.31           72.56       71.92   \n",
       "2              llama3-70b              68.94           72.58       70.69   \n",
       "3            AzLlama-150M              67.78           69.01       68.35   \n",
       "4  mixtral-8x22b-instruct              65.72           68.93       67.26   \n",
       "5     llama-3-8b-instruct              64.50           69.15       66.72   \n",
       "6             gemma-7b-it              64.75           68.78       66.66   \n",
       "7   mixtral-8x7b-instruct              65.52           66.89       66.18   \n",
       "8     mistral-7b-instruct              63.34           64.44       63.87   \n",
       "9              llama2_70b              59.08           63.40       61.13   \n",
       "\n",
       "  # Parameters # Training Tokens  Access Type  semantic_scores   chrf  \n",
       "0          >1T    No Information  proprietary            85.14  42.73  \n",
       "1        >100B  \"No Information\"  proprietary            84.96  41.06  \n",
       "2          70B               15T  open-weight            81.55  40.49  \n",
       "3         150M                9B  open-weight            82.87  34.82  \n",
       "4        8x22B    No Information  open-weight            82.49  33.05  \n",
       "5           8B               15T  open-weight            82.30  33.29  \n",
       "6           7B                6T  open-weight            60.03  31.47  \n",
       "7         8x7B    No Information  open-weight            75.60  31.51  \n",
       "8           7B                8T  open-weight            59.57  26.10  \n",
       "9          70B                2T  open-weight            65.78  23.36  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('evaluation.csv')\n",
    "df['Average Precision'] = df['Average Precision']*100\n",
    "df['Average Recall'] = df['Average Recall']*100\n",
    "df['Average F1'] = df['Average F1']*100\n",
    "df['semantic_scores'] = df['semantic_scores']*100\n",
    "df = df.round(2)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Model', 'Average Precision', 'Average Recall', 'Average F1',\n",
       "       '# Parameters', '# Training Tokens', 'Access Type', 'semantic_scores',\n",
       "       'chrf'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Contextual Alignment Precision</th>\n",
       "      <th>Contextual Alignment Recall</th>\n",
       "      <th>Contextual Alignment Score F1</th>\n",
       "      <th># Parameters</th>\n",
       "      <th># Training Tokens</th>\n",
       "      <th>Access Type</th>\n",
       "      <th>Semantic Scores</th>\n",
       "      <th>Character n-gram F-score (CHRF)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>claude_haiku</td>\n",
       "      <td>73.02</td>\n",
       "      <td>74.00</td>\n",
       "      <td>73.50</td>\n",
       "      <td>&gt;1T</td>\n",
       "      <td>No Information</td>\n",
       "      <td>proprietary</td>\n",
       "      <td>85.14</td>\n",
       "      <td>42.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt35-turbo</td>\n",
       "      <td>71.31</td>\n",
       "      <td>72.56</td>\n",
       "      <td>71.92</td>\n",
       "      <td>&gt;100B</td>\n",
       "      <td>\"No Information\"</td>\n",
       "      <td>proprietary</td>\n",
       "      <td>84.96</td>\n",
       "      <td>41.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>llama3-70b</td>\n",
       "      <td>68.94</td>\n",
       "      <td>72.58</td>\n",
       "      <td>70.69</td>\n",
       "      <td>70B</td>\n",
       "      <td>15T</td>\n",
       "      <td>open-weight</td>\n",
       "      <td>81.55</td>\n",
       "      <td>40.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AzLlama-150M</td>\n",
       "      <td>67.78</td>\n",
       "      <td>69.01</td>\n",
       "      <td>68.35</td>\n",
       "      <td>150M</td>\n",
       "      <td>9B</td>\n",
       "      <td>open-weight</td>\n",
       "      <td>82.87</td>\n",
       "      <td>34.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mixtral-8x22b-instruct</td>\n",
       "      <td>65.72</td>\n",
       "      <td>68.93</td>\n",
       "      <td>67.26</td>\n",
       "      <td>8x22B</td>\n",
       "      <td>No Information</td>\n",
       "      <td>open-weight</td>\n",
       "      <td>82.49</td>\n",
       "      <td>33.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>llama-3-8b-instruct</td>\n",
       "      <td>64.50</td>\n",
       "      <td>69.15</td>\n",
       "      <td>66.72</td>\n",
       "      <td>8B</td>\n",
       "      <td>15T</td>\n",
       "      <td>open-weight</td>\n",
       "      <td>82.30</td>\n",
       "      <td>33.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gemma-7b-it</td>\n",
       "      <td>64.75</td>\n",
       "      <td>68.78</td>\n",
       "      <td>66.66</td>\n",
       "      <td>7B</td>\n",
       "      <td>6T</td>\n",
       "      <td>open-weight</td>\n",
       "      <td>60.03</td>\n",
       "      <td>31.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mixtral-8x7b-instruct</td>\n",
       "      <td>65.52</td>\n",
       "      <td>66.89</td>\n",
       "      <td>66.18</td>\n",
       "      <td>8x7B</td>\n",
       "      <td>No Information</td>\n",
       "      <td>open-weight</td>\n",
       "      <td>75.60</td>\n",
       "      <td>31.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mistral-7b-instruct</td>\n",
       "      <td>63.34</td>\n",
       "      <td>64.44</td>\n",
       "      <td>63.87</td>\n",
       "      <td>7B</td>\n",
       "      <td>8T</td>\n",
       "      <td>open-weight</td>\n",
       "      <td>59.57</td>\n",
       "      <td>26.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>llama2_70b</td>\n",
       "      <td>59.08</td>\n",
       "      <td>63.40</td>\n",
       "      <td>61.13</td>\n",
       "      <td>70B</td>\n",
       "      <td>2T</td>\n",
       "      <td>open-weight</td>\n",
       "      <td>65.78</td>\n",
       "      <td>23.36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Model  Contextual Alignment Precision  \\\n",
       "0            claude_haiku                           73.02   \n",
       "1             gpt35-turbo                           71.31   \n",
       "2              llama3-70b                           68.94   \n",
       "3            AzLlama-150M                           67.78   \n",
       "4  mixtral-8x22b-instruct                           65.72   \n",
       "5     llama-3-8b-instruct                           64.50   \n",
       "6             gemma-7b-it                           64.75   \n",
       "7   mixtral-8x7b-instruct                           65.52   \n",
       "8     mistral-7b-instruct                           63.34   \n",
       "9              llama2_70b                           59.08   \n",
       "\n",
       "   Contextual Alignment Recall  Contextual Alignment Score F1 # Parameters  \\\n",
       "0                        74.00                          73.50          >1T   \n",
       "1                        72.56                          71.92        >100B   \n",
       "2                        72.58                          70.69          70B   \n",
       "3                        69.01                          68.35         150M   \n",
       "4                        68.93                          67.26        8x22B   \n",
       "5                        69.15                          66.72           8B   \n",
       "6                        68.78                          66.66           7B   \n",
       "7                        66.89                          66.18         8x7B   \n",
       "8                        64.44                          63.87           7B   \n",
       "9                        63.40                          61.13          70B   \n",
       "\n",
       "  # Training Tokens  Access Type  Semantic Scores  \\\n",
       "0    No Information  proprietary            85.14   \n",
       "1  \"No Information\"  proprietary            84.96   \n",
       "2               15T  open-weight            81.55   \n",
       "3                9B  open-weight            82.87   \n",
       "4    No Information  open-weight            82.49   \n",
       "5               15T  open-weight            82.30   \n",
       "6                6T  open-weight            60.03   \n",
       "7    No Information  open-weight            75.60   \n",
       "8                8T  open-weight            59.57   \n",
       "9                2T  open-weight            65.78   \n",
       "\n",
       "   Character n-gram F-score (CHRF)  \n",
       "0                            42.73  \n",
       "1                            41.06  \n",
       "2                            40.49  \n",
       "3                            34.82  \n",
       "4                            33.05  \n",
       "5                            33.29  \n",
       "6                            31.47  \n",
       "7                            31.51  \n",
       "8                            26.10  \n",
       "9                            23.36  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns = ['Model', 'Contextual Alignment Precision', 'Contextual Alignment Recall', 'Contextual Alignment Score F1',\n",
    "              '# Parameters', '# Training Tokens', 'Access Type',\n",
    "        'Semantic Scores', 'Character n-gram F-score (CHRF)', ]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Organization'] = [\n",
    "    'Anthropic',\n",
    "    \"OpenAI\",\n",
    "    \"Meta\",\n",
    "    'ADA & GW',\n",
    "    'Mistral AI',\n",
    "    'Meta',\n",
    "    'Google',\n",
    "    'Mistral AI',   \n",
    "    'Mistral AI',\n",
    "    \"Meta\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Contextual Alignment Precision</th>\n",
       "      <th>Contextual Alignment Recall</th>\n",
       "      <th>Contextual Alignment Score F1</th>\n",
       "      <th># Parameters</th>\n",
       "      <th># Training Tokens</th>\n",
       "      <th>Access Type</th>\n",
       "      <th>Semantic Scores</th>\n",
       "      <th>Character n-gram F-score (CHRF)</th>\n",
       "      <th>Organization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>claude_haiku</td>\n",
       "      <td>73.02</td>\n",
       "      <td>74.00</td>\n",
       "      <td>73.50</td>\n",
       "      <td>&gt;1T</td>\n",
       "      <td>No Information</td>\n",
       "      <td>proprietary</td>\n",
       "      <td>85.14</td>\n",
       "      <td>42.73</td>\n",
       "      <td>Anthropic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt35-turbo</td>\n",
       "      <td>71.31</td>\n",
       "      <td>72.56</td>\n",
       "      <td>71.92</td>\n",
       "      <td>&gt;100B</td>\n",
       "      <td>\"No Information\"</td>\n",
       "      <td>proprietary</td>\n",
       "      <td>84.96</td>\n",
       "      <td>41.06</td>\n",
       "      <td>OpenAI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>llama3-70b</td>\n",
       "      <td>68.94</td>\n",
       "      <td>72.58</td>\n",
       "      <td>70.69</td>\n",
       "      <td>70B</td>\n",
       "      <td>15T</td>\n",
       "      <td>open-weight</td>\n",
       "      <td>81.55</td>\n",
       "      <td>40.49</td>\n",
       "      <td>Meta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AzLlama-150M</td>\n",
       "      <td>67.78</td>\n",
       "      <td>69.01</td>\n",
       "      <td>68.35</td>\n",
       "      <td>150M</td>\n",
       "      <td>9B</td>\n",
       "      <td>open-weight</td>\n",
       "      <td>82.87</td>\n",
       "      <td>34.82</td>\n",
       "      <td>ADA &amp; GW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mixtral-8x22b-instruct</td>\n",
       "      <td>65.72</td>\n",
       "      <td>68.93</td>\n",
       "      <td>67.26</td>\n",
       "      <td>8x22B</td>\n",
       "      <td>No Information</td>\n",
       "      <td>open-weight</td>\n",
       "      <td>82.49</td>\n",
       "      <td>33.05</td>\n",
       "      <td>Mistral AI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>llama-3-8b-instruct</td>\n",
       "      <td>64.50</td>\n",
       "      <td>69.15</td>\n",
       "      <td>66.72</td>\n",
       "      <td>8B</td>\n",
       "      <td>15T</td>\n",
       "      <td>open-weight</td>\n",
       "      <td>82.30</td>\n",
       "      <td>33.29</td>\n",
       "      <td>Meta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gemma-7b-it</td>\n",
       "      <td>64.75</td>\n",
       "      <td>68.78</td>\n",
       "      <td>66.66</td>\n",
       "      <td>7B</td>\n",
       "      <td>6T</td>\n",
       "      <td>open-weight</td>\n",
       "      <td>60.03</td>\n",
       "      <td>31.47</td>\n",
       "      <td>Google</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mixtral-8x7b-instruct</td>\n",
       "      <td>65.52</td>\n",
       "      <td>66.89</td>\n",
       "      <td>66.18</td>\n",
       "      <td>8x7B</td>\n",
       "      <td>No Information</td>\n",
       "      <td>open-weight</td>\n",
       "      <td>75.60</td>\n",
       "      <td>31.51</td>\n",
       "      <td>Mistral AI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mistral-7b-instruct</td>\n",
       "      <td>63.34</td>\n",
       "      <td>64.44</td>\n",
       "      <td>63.87</td>\n",
       "      <td>7B</td>\n",
       "      <td>8T</td>\n",
       "      <td>open-weight</td>\n",
       "      <td>59.57</td>\n",
       "      <td>26.10</td>\n",
       "      <td>Mistral AI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>llama2_70b</td>\n",
       "      <td>59.08</td>\n",
       "      <td>63.40</td>\n",
       "      <td>61.13</td>\n",
       "      <td>70B</td>\n",
       "      <td>2T</td>\n",
       "      <td>open-weight</td>\n",
       "      <td>65.78</td>\n",
       "      <td>23.36</td>\n",
       "      <td>Meta</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Model  Contextual Alignment Precision  \\\n",
       "0            claude_haiku                           73.02   \n",
       "1             gpt35-turbo                           71.31   \n",
       "2              llama3-70b                           68.94   \n",
       "3            AzLlama-150M                           67.78   \n",
       "4  mixtral-8x22b-instruct                           65.72   \n",
       "5     llama-3-8b-instruct                           64.50   \n",
       "6             gemma-7b-it                           64.75   \n",
       "7   mixtral-8x7b-instruct                           65.52   \n",
       "8     mistral-7b-instruct                           63.34   \n",
       "9              llama2_70b                           59.08   \n",
       "\n",
       "   Contextual Alignment Recall  Contextual Alignment Score F1 # Parameters  \\\n",
       "0                        74.00                          73.50          >1T   \n",
       "1                        72.56                          71.92        >100B   \n",
       "2                        72.58                          70.69          70B   \n",
       "3                        69.01                          68.35         150M   \n",
       "4                        68.93                          67.26        8x22B   \n",
       "5                        69.15                          66.72           8B   \n",
       "6                        68.78                          66.66           7B   \n",
       "7                        66.89                          66.18         8x7B   \n",
       "8                        64.44                          63.87           7B   \n",
       "9                        63.40                          61.13          70B   \n",
       "\n",
       "  # Training Tokens  Access Type  Semantic Scores  \\\n",
       "0    No Information  proprietary            85.14   \n",
       "1  \"No Information\"  proprietary            84.96   \n",
       "2               15T  open-weight            81.55   \n",
       "3                9B  open-weight            82.87   \n",
       "4    No Information  open-weight            82.49   \n",
       "5               15T  open-weight            82.30   \n",
       "6                6T  open-weight            60.03   \n",
       "7    No Information  open-weight            75.60   \n",
       "8                8T  open-weight            59.57   \n",
       "9                2T  open-weight            65.78   \n",
       "\n",
       "   Character n-gram F-score (CHRF) Organization  \n",
       "0                            42.73    Anthropic  \n",
       "1                            41.06       OpenAI  \n",
       "2                            40.49         Meta  \n",
       "3                            34.82     ADA & GW  \n",
       "4                            33.05   Mistral AI  \n",
       "5                            33.29         Meta  \n",
       "6                            31.47       Google  \n",
       "7                            31.51   Mistral AI  \n",
       "8                            26.10   Mistral AI  \n",
       "9                            23.36         Meta  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Model', 'Contextual Alignment Precision',\n",
       "       'Contextual Alignment Recall', 'Contextual Alignment Score F1',\n",
       "       '# Parameters', '# Training Tokens', 'Access Type', 'Semantic Scores',\n",
       "       'Character n-gram F-score (CHRF)', 'Organization'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Contextual Alignment Precision</th>\n",
       "      <th>Contextual Alignment Recall</th>\n",
       "      <th>Contextual Alignment Score F1</th>\n",
       "      <th># Parameters</th>\n",
       "      <th># Training Tokens</th>\n",
       "      <th>Access Type</th>\n",
       "      <th>Organization</th>\n",
       "      <th>Semantic Scores</th>\n",
       "      <th>Character n-gram F-score (CHRF)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>claude_haiku</td>\n",
       "      <td>73.02</td>\n",
       "      <td>74.00</td>\n",
       "      <td>73.50</td>\n",
       "      <td>&gt;1T</td>\n",
       "      <td>No Information</td>\n",
       "      <td>proprietary</td>\n",
       "      <td>Anthropic</td>\n",
       "      <td>85.14</td>\n",
       "      <td>42.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt35-turbo</td>\n",
       "      <td>71.31</td>\n",
       "      <td>72.56</td>\n",
       "      <td>71.92</td>\n",
       "      <td>&gt;100B</td>\n",
       "      <td>\"No Information\"</td>\n",
       "      <td>proprietary</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>84.96</td>\n",
       "      <td>41.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>llama3-70b</td>\n",
       "      <td>68.94</td>\n",
       "      <td>72.58</td>\n",
       "      <td>70.69</td>\n",
       "      <td>70B</td>\n",
       "      <td>15T</td>\n",
       "      <td>open-weight</td>\n",
       "      <td>Meta</td>\n",
       "      <td>81.55</td>\n",
       "      <td>40.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AzLlama-150M</td>\n",
       "      <td>67.78</td>\n",
       "      <td>69.01</td>\n",
       "      <td>68.35</td>\n",
       "      <td>150M</td>\n",
       "      <td>9B</td>\n",
       "      <td>open-weight</td>\n",
       "      <td>ADA &amp; GW</td>\n",
       "      <td>82.87</td>\n",
       "      <td>34.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mixtral-8x22b-instruct</td>\n",
       "      <td>65.72</td>\n",
       "      <td>68.93</td>\n",
       "      <td>67.26</td>\n",
       "      <td>8x22B</td>\n",
       "      <td>No Information</td>\n",
       "      <td>open-weight</td>\n",
       "      <td>Mistral AI</td>\n",
       "      <td>82.49</td>\n",
       "      <td>33.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>llama-3-8b-instruct</td>\n",
       "      <td>64.50</td>\n",
       "      <td>69.15</td>\n",
       "      <td>66.72</td>\n",
       "      <td>8B</td>\n",
       "      <td>15T</td>\n",
       "      <td>open-weight</td>\n",
       "      <td>Meta</td>\n",
       "      <td>82.30</td>\n",
       "      <td>33.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gemma-7b-it</td>\n",
       "      <td>64.75</td>\n",
       "      <td>68.78</td>\n",
       "      <td>66.66</td>\n",
       "      <td>7B</td>\n",
       "      <td>6T</td>\n",
       "      <td>open-weight</td>\n",
       "      <td>Google</td>\n",
       "      <td>60.03</td>\n",
       "      <td>31.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mixtral-8x7b-instruct</td>\n",
       "      <td>65.52</td>\n",
       "      <td>66.89</td>\n",
       "      <td>66.18</td>\n",
       "      <td>8x7B</td>\n",
       "      <td>No Information</td>\n",
       "      <td>open-weight</td>\n",
       "      <td>Mistral AI</td>\n",
       "      <td>75.60</td>\n",
       "      <td>31.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mistral-7b-instruct</td>\n",
       "      <td>63.34</td>\n",
       "      <td>64.44</td>\n",
       "      <td>63.87</td>\n",
       "      <td>7B</td>\n",
       "      <td>8T</td>\n",
       "      <td>open-weight</td>\n",
       "      <td>Mistral AI</td>\n",
       "      <td>59.57</td>\n",
       "      <td>26.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>llama2_70b</td>\n",
       "      <td>59.08</td>\n",
       "      <td>63.40</td>\n",
       "      <td>61.13</td>\n",
       "      <td>70B</td>\n",
       "      <td>2T</td>\n",
       "      <td>open-weight</td>\n",
       "      <td>Meta</td>\n",
       "      <td>65.78</td>\n",
       "      <td>23.36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Model  Contextual Alignment Precision  \\\n",
       "0            claude_haiku                           73.02   \n",
       "1             gpt35-turbo                           71.31   \n",
       "2              llama3-70b                           68.94   \n",
       "3            AzLlama-150M                           67.78   \n",
       "4  mixtral-8x22b-instruct                           65.72   \n",
       "5     llama-3-8b-instruct                           64.50   \n",
       "6             gemma-7b-it                           64.75   \n",
       "7   mixtral-8x7b-instruct                           65.52   \n",
       "8     mistral-7b-instruct                           63.34   \n",
       "9              llama2_70b                           59.08   \n",
       "\n",
       "   Contextual Alignment Recall  Contextual Alignment Score F1 # Parameters  \\\n",
       "0                        74.00                          73.50          >1T   \n",
       "1                        72.56                          71.92        >100B   \n",
       "2                        72.58                          70.69          70B   \n",
       "3                        69.01                          68.35         150M   \n",
       "4                        68.93                          67.26        8x22B   \n",
       "5                        69.15                          66.72           8B   \n",
       "6                        68.78                          66.66           7B   \n",
       "7                        66.89                          66.18         8x7B   \n",
       "8                        64.44                          63.87           7B   \n",
       "9                        63.40                          61.13          70B   \n",
       "\n",
       "  # Training Tokens  Access Type Organization  Semantic Scores  \\\n",
       "0    No Information  proprietary    Anthropic            85.14   \n",
       "1  \"No Information\"  proprietary       OpenAI            84.96   \n",
       "2               15T  open-weight         Meta            81.55   \n",
       "3                9B  open-weight     ADA & GW            82.87   \n",
       "4    No Information  open-weight   Mistral AI            82.49   \n",
       "5               15T  open-weight         Meta            82.30   \n",
       "6                6T  open-weight       Google            60.03   \n",
       "7    No Information  open-weight   Mistral AI            75.60   \n",
       "8                8T  open-weight   Mistral AI            59.57   \n",
       "9                2T  open-weight         Meta            65.78   \n",
       "\n",
       "   Character n-gram F-score (CHRF)  \n",
       "0                            42.73  \n",
       "1                            41.06  \n",
       "2                            40.49  \n",
       "3                            34.82  \n",
       "4                            33.05  \n",
       "5                            33.29  \n",
       "6                            31.47  \n",
       "7                            31.51  \n",
       "8                            26.10  \n",
       "9                            23.36  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[\n",
    "    [\n",
    "        'Model', \n",
    "     'Contextual Alignment Precision', 'Contextual Alignment Recall', 'Contextual Alignment Score F1',\n",
    "       '# Parameters', '# Training Tokens', 'Access Type', 'Organization',\n",
    "       'Semantic Scores', 'Character n-gram F-score (CHRF)', \n",
    "       ]\n",
    "]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('evaluation_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "polygraf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
