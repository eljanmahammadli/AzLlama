{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/eljan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import unicodedata\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Ensure you've downloaded the NLTK Punkt tokenizer for sentence splitting.\n",
    "nltk.download('punkt')\n",
    "\n",
    "def remove_accents(input_str):\n",
    "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
    "    return \"\".join([c for c in nfkd_form if not unicodedata.combining(c)])\n",
    "\n",
    "def normalize_numbers(text):\n",
    "    return ''.join('0' if c.isdigit() else c for c in text)\n",
    "\n",
    "def normalize_punctuation(text):\n",
    "    # Adjust these rules as necessary for Azerbaijani text.\n",
    "    text = re.sub(r\"[“”]\", '\"', text)\n",
    "    text = re.sub(r\"[‘’]\", \"'\", text)\n",
    "    text = re.sub(r\"([.,;:!?])([^\\s])\", r\"\\1 \\2\", text)\n",
    "    return text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Lowercase\n",
    "    text = remove_accents(text)  # Remove accents\n",
    "    text = normalize_numbers(text)  # Normalize numbers\n",
    "    text = normalize_punctuation(text)  # Normalize punctuation\n",
    "    return text\n",
    "\n",
    "def process_documents_into_single_file(documents, output_file_name):\n",
    "    with open(output_file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "        for document in documents:\n",
    "            # Split document into sentences\n",
    "            sentences = sent_tokenize(document)\n",
    "            for sentence in sentences:\n",
    "                # Preprocess each sentence\n",
    "                processed_sentence = preprocess_text(sentence)\n",
    "                f.write(processed_sentence + \"\\n\")\n",
    "\n",
    "# Example usage\n",
    "# documents = [\"Your document text here. Another sentence here.\", \"Second document's text.\"]\n",
    "# process_documents_into_single_file(documents, \"all_processed_sentences.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kenlm\n",
    "import math\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "# Load your KenLM model (make sure the path to your .arpa or .klm file is correct)\n",
    "model = kenlm.Model('output_model.klm')  # or 'output_model.arpa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = \"\"\"Azərbaycan və ya rəsmi adı ilə Azərbaycan Respublikası — Şərqi Avropa və Qərbi Asiyanın sərhəddində yerləşən transkontinental ölkə. Azərbaycan Xəzər dənizi hövzəsinin qərbində, Cənubi Qafqazda yerləşir. Şimaldan Rusiya (Dağıstan),[7] şimal-qərbdən Gürcüstan, qərbdən Ermənistan, cənub-qərbdən Türkiyə və cənubdan İran ilə həmsərhəddir.[8] Azərbaycanın eksklavı olan Naxçıvan Muxtar Respublikası Ermənistanla şimal-şərqdə, İranla qərbdə və Türkiyə ilə şimal-qərbdən həmsərhəddir. Azərbaycan ərazisinin bir hissəsi (Dağlıq Qarabağ bölgəsi və ona bitişik Before receiving any explanation, the Mongols marched through Song territory to enter \"\"\"\n",
    "\n",
    "document2 = \"\"\"Before the Mongol–Jin War escalated, an envoy from the Song dynasty of China arrived at the court of the Mongols, perhaps to negotiate a united offensive against the Jin dynasty, who the Song had previously fought during the Jin–Song Wars. Although Genghis Khan refused, on his death in 1227 he bequeathed a plan to attack the Jin capital by passing through Song territory. Subsequently, a Mongol ambassador was killed by the Song governor in uncertain circumstances.[3] Before receiving any explanation, the Mongols marched through Song territory to enter the Jin's redoubt in Henan.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-2243.132080078125, 24.197931618547283)\n"
     ]
    }
   ],
   "source": [
    "# TODO: iterate over sentences and then aggregate the perplexity scores\n",
    "# document = input(\"Here: \")\n",
    "def evaluate1(model, document):\n",
    "    document = preprocess_text(document)\n",
    "    log_prob = model.score(document)\n",
    "    word_count = len(word_tokenize(document))\n",
    "    perplexity = math.exp(-log_prob / word_count)\n",
    "    return log_prob, perplexity\n",
    "\n",
    "# print(evaluate1(model, document))\n",
    "# print(evaluate1(model, document2))\n",
    "print(evaluate1(model, input(\"Here: \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-2531.442458152771, 30.73792508630148)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def evaluate(model, document):\n",
    "    # Assuming preprocess_text does necessary preprocessing like lowercasing, etc.\n",
    "    document = preprocess_text(document)  \n",
    "    sentences = sent_tokenize(document)\n",
    "    \n",
    "    total_log_prob = 0.0\n",
    "    total_words = 0\n",
    "    for sentence in sentences:\n",
    "        # Tokenize the sentence into words to count them, assuming KenLM model was trained on tokenized text.\n",
    "        words = word_tokenize(sentence)\n",
    "        total_words += len(words) + 1  # +1 for EOS\n",
    "        \n",
    "        # Score the sentence with bos and eos tokens\n",
    "        log_prob = model.score(' '.join(words))\n",
    "        total_log_prob += log_prob\n",
    "\n",
    "    # Compute perplexity\n",
    "    perplexity = math.exp(-total_log_prob / total_words)\n",
    "    return total_log_prob, perplexity\n",
    "\n",
    "\n",
    "evaluate(model, input(\"Here: \"))\n",
    "# print(evaluate(model, document))\n",
    "# print(evaluate(model, document2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "polygraf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
